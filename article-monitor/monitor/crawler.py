"""
çˆ¬å–ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å–ï¼Œå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œé‡è¯•æœºåˆ¶ï¼Œé˜²åçˆ¬
"""
import asyncio
import threading
import logging
from datetime import datetime
from typing import List, Dict, Optional
from .database import (
    get_all_articles, add_read_count, get_latest_read_count,
    get_latest_read_counts_batch, update_article_title, update_article_status
)
from .extractors import extract_read_count, extract_article_info, create_shared_crawler
from urllib.parse import urlparse
from .config import (
    SUPPORTED_SITES, CRAWL_CONCURRENCY, CRAWL_DELAY, 
    CRAWL_MAX_RETRIES, CRAWL_RETRY_DELAY, CRAWL_RETRY_BACKOFF,
    ANTI_SCRAPING_ENABLED, ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_MIN_DELAY, ANTI_SCRAPING_MAX_DELAY,
    is_platform_allowed
)
from .anti_scraping import get_anti_scraping_manager, reset_anti_scraping_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€è¿›åº¦çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤ï¼‰
_crawl_progress_lock = threading.Lock()
_crawl_progress = {
    'is_running': False,
    'total': 0,
    'current': 0,
    'success': 0,
    'failed': 0,
    'retried': 0,
    'current_url': None,
    'start_time': None,
    'end_time': None
}

# å…¨å±€åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤ï¼‰
_stop_signal_lock = threading.Lock()
_stop_signal = False

def stop_crawling():
    """åœæ­¢çˆ¬å–ä»»åŠ¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _stop_signal
    with _stop_signal_lock:
        _stop_signal = True
    logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢çˆ¬å–...")

def get_crawl_progress():
    """è·å–çˆ¬å–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with _crawl_progress_lock:
        return _crawl_progress.copy()

def reset_crawl_progress():
    """é‡ç½®çˆ¬å–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _crawl_progress
    with _crawl_progress_lock:
        _crawl_progress = {
            'is_running': False,
            'total': 0,
            'current': 0,
            'success': 0,
            'failed': 0,
            'retried': 0,
            'current_url': None,
            'start_time': None,
            'end_time': None
        }

def _is_retryable_error(error: Exception) -> bool:
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
    error_str = str(error).lower()
    retryable_keywords = [
        'timeout', 'connection', 'network', 'temporary',
        '503', '502', '504', '429',  # HTTPé”™è¯¯ç 
        'econnrefused', 'econnreset', 'etimedout',
        'ssl', 'certificate', 'handshake'
    ]
    return any(keyword in error_str for keyword in retryable_keywords)

async def crawl_article_with_retry(article: dict, crawler=None, semaphore=None, max_retries: int = None, skip_retry: bool = False) -> bool:
    """çˆ¬å–å•ç¯‡æ–‡ç« ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        semaphore: å¯é€‰çš„å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        skip_retry: æ˜¯å¦è·³è¿‡é‡è¯•ï¼ˆç”¨äºç¬¬ä¸€è½®çˆ¬å–ï¼Œåªå°è¯•ä¸€æ¬¡ï¼‰
    """
    if max_retries is None:
        max_retries = CRAWL_MAX_RETRIES
    
    url = article['url']
    article_id = article['id']
    site = article.get('site')
    
    # æ£€æŸ¥å¹³å°æ˜¯å¦åœ¨ç™½åå•ä¸­ï¼ˆåŒé‡ä¿é™©ï¼‰
    if not is_platform_allowed(site):
        logger.debug(f"â­ï¸  è·³è¿‡éç™½åå•å¹³å°æ–‡ç« : {url} (å¹³å°: {site})")
        return False
    
    # æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _stop_signal_lock:
        if _stop_signal:
            return False
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼ˆä¼˜åŒ–ï¼šæ¶ˆé™¤é‡å¤ä»£ç ï¼‰
    async def _do_crawl():
        return await _crawl_with_retry(
            article, crawler, 
            max_retries if not skip_retry else 0,
            mark_error_on_fail=not skip_retry
        )
    
    if semaphore:
        async with semaphore:
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨è·å–ä¿¡å·é‡åï¼‰
            with _stop_signal_lock:
                if _stop_signal:
                    return False
            return await _do_crawl()
    else:
        return await _do_crawl()

async def _crawl_with_retry(article: dict, crawler=None, max_retries: int = 3, mark_error_on_fail: bool = True) -> bool:
    """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–é€»è¾‘ï¼ˆåŒæ—¶æ›´æ–°æ ‡é¢˜ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„æµè§ˆå™¨å®ä¾‹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        mark_error_on_fail: å¤±è´¥æ—¶æ˜¯å¦ç«‹å³æ ‡è®°ä¸ºERRORï¼ˆç¬¬ä¸€è½®ä¸æ ‡è®°ï¼Œç¬¬äºŒè½®æ ‡è®°ï¼‰
    """
    url = article['url']
    article_id = article['id']
    current_title = article.get('title', '')
    
    last_error = None
    
    for attempt in range(max_retries + 1):  # 0åˆ°max_retriesï¼Œå…±max_retries+1æ¬¡å°è¯•
        try:
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt > 0:
                # æŒ‡æ•°é€€é¿ï¼šå»¶è¿Ÿæ—¶é—´ = åŸºç¡€å»¶è¿Ÿ * (é€€é¿å€æ•° ^ å°è¯•æ¬¡æ•°)
                delay = CRAWL_RETRY_DELAY * (CRAWL_RETRY_BACKOFF ** (attempt - 1))
                logger.info(f"ğŸ”„ é‡è¯• {attempt}/{max_retries}: {url} (ç­‰å¾… {delay:.1f}ç§’)")
                await asyncio.sleep(delay)
                
                # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨ç¡çœ æœŸé—´å¯èƒ½æ”¶åˆ°äº†åœæ­¢ä¿¡å·ï¼‰
                with _stop_signal_lock:
                    if _stop_signal:
                        logger.info(f"ğŸ›‘ ä»»åŠ¡å·²åœæ­¢: {url}")
                        return False
                    
                with _crawl_progress_lock:
                    _crawl_progress['retried'] += 1
            
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
            with _stop_signal_lock:
                if _stop_signal:
                    return False
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆåŒæ—¶è·å–é˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
            info = await extract_article_info(url, crawler)
            count = info.get('read_count')
            new_title = info.get('title')
            
            # æ›´æ–°æ–‡ç« æ ‡é¢˜ï¼ˆå¦‚æœæœ‰æ–°æ ‡é¢˜ä¸”ä¸å½“å‰æ ‡é¢˜ä¸åŒï¼‰
            if new_title and new_title != current_title:
                if update_article_title(article_id, new_title):
                    logger.info(f"ğŸ“ æ›´æ–°æ ‡é¢˜: {new_title[:30]}...")
            
            if count is None:
                # å¦‚æœæå–å¤±è´¥ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < max_retries:
                    logger.debug(f"âš ï¸  æå–å¤±è´¥ï¼Œå°†é‡è¯•: {url} (å°è¯• {attempt + 1}/{max_retries + 1})")
                    continue
                else:
                    logger.warning(f"âŒ æ— æ³•æå–é˜…è¯»æ•°: {url} (å·²é‡è¯• {max_retries} æ¬¡)")
                    # æå–å¤±è´¥æ—¶ä¸æ ‡è®°ERRORï¼ˆç­‰å¾…é›†ä¸­é‡è¯•ï¼‰
                    if mark_error_on_fail:
                        update_article_status(article_id, 'ERROR', 'æ— æ³•æå–é˜…è¯»æ•°')
                    return False
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆé¿å…é‡å¤ç›¸åŒæ•°æ®ï¼‰
            # ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„æœ€æ–°é˜…è¯»æ•°ï¼Œé¿å…æ•°æ®åº“æŸ¥è¯¢
            latest_count = article.get('_latest_count')
            if latest_count is None:
                # å¦‚æœé¢„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢
                latest = get_latest_read_count(article_id)
                latest_count = latest['count'] if latest else None
            
            if latest_count is not None and latest_count == count:
                logger.debug(f"âœ“ é˜…è¯»æ•°æœªå˜åŒ–: {url} ({count})")
                # å³ä½¿æœªå˜åŒ–ï¼Œä¹Ÿæ›´æ–°çŠ¶æ€ä¸ºæˆåŠŸï¼ˆè¡¨ç¤ºçˆ¬å–æ­£å¸¸ï¼‰
                update_article_status(article_id, 'OK')
                return True
            
            # ä¿å­˜é˜…è¯»æ•°
            add_read_count(article_id, count)
            
            # æ›´æ–°çŠ¶æ€ä¸ºæˆåŠŸ
            update_article_status(article_id, 'OK')
            
            if attempt > 0:
                logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url} -> {count} (å°è¯• {attempt + 1} æ¬¡)")
            else:
                logger.info(f"âœ… æ›´æ–°æˆåŠŸ: {url} -> {count}")
            return True
            
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•
            is_retryable = _is_retryable_error(e)
            
            if is_retryable and attempt < max_retries:
                logger.warning(f"âš ï¸  å¯é‡è¯•é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {url} - {str(e)[:100]}")
                continue
            else:
                # ä¸å¯é‡è¯•æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                error_msg = str(e)[:100]
                if 'timeout' in error_str or 'connection' in error_str:
                    logger.error(f"â±ï¸  ç½‘ç»œé”™è¯¯ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                else:
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆæ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ ‡è®°ï¼‰
                # ç¬¬ä¸€è½®ä¸æ ‡è®°ERRORï¼ˆç­‰å¾…é›†ä¸­é‡è¯•ï¼‰ï¼Œç¬¬äºŒè½®æ‰æ ‡è®°
                if mark_error_on_fail and (not is_retryable or attempt >= max_retries):
                    update_article_status(article_id, 'ERROR', str(e))
                
                return False
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    final_error = str(last_error) if last_error else 'æœªçŸ¥é”™è¯¯'
    logger.error(f"âŒ çˆ¬å–æœ€ç»ˆå¤±è´¥ {url}: {final_error[:100]}")
    if mark_error_on_fail:
        update_article_status(article_id, 'ERROR', final_error)
    return False


async def crawl_all_articles():
    """çˆ¬å–æ‰€æœ‰æ–‡ç«  - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å– + é‡è¯•æœºåˆ¶"""
    global _crawl_progress, _stop_signal
    
    # é‡ç½®åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _stop_signal_lock:
        _stop_signal = False
    
    articles = get_all_articles()
    if not articles:
        logger.info("æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ–‡ç« ")
        reset_crawl_progress()
        return
    
    # è¿‡æ»¤ï¼šåªçˆ¬å–ç™½åå•ä¸­çš„å¹³å°
    filtered_articles = []
    skipped_count = 0
    for article in articles:
        site = article.get('site')
        if is_platform_allowed(site):
            filtered_articles.append(article)
        else:
            skipped_count += 1
            logger.debug(f"â­ï¸  è·³è¿‡éç™½åå•å¹³å°: {article.get('url')} (å¹³å°: {site})")
    
    if skipped_count > 0:
        logger.info(f"â­ï¸  å·²è·³è¿‡ {skipped_count} ç¯‡éç™½åå•å¹³å°æ–‡ç« ")
    
    if not filtered_articles:
        logger.info("æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ–‡ç« ï¼ˆæ‰€æœ‰æ–‡ç« éƒ½ä¸åœ¨ç™½åå•ä¸­ï¼‰")
        reset_crawl_progress()
        return
    
    articles = filtered_articles
    
    # æ‰¹é‡è·å–æœ€æ–°é˜…è¯»æ•°ï¼ˆä¼˜åŒ–ï¼šé¿å…åœ¨çˆ¬å–å¾ªç¯ä¸­é€ä¸ªæŸ¥è¯¢ï¼‰
    article_ids = [a['id'] for a in articles]
    latest_counts = get_latest_read_counts_batch(article_ids)
    
    # å°†æœ€æ–°é˜…è¯»æ•°æ·»åŠ åˆ°æ–‡ç« å­—å…¸ä¸­ï¼Œé¿å…åœ¨çˆ¬å–æ—¶é‡å¤æŸ¥è¯¢
    for article in articles:
        latest = latest_counts.get(article['id'])
        article['_latest_count'] = latest['count'] if latest else None
    
    # åˆå§‹åŒ–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _crawl_progress_lock:
        _crawl_progress['is_running'] = True
        _crawl_progress['total'] = len(articles)
        _crawl_progress['current'] = 0
        _crawl_progress['success'] = 0
        _crawl_progress['failed'] = 0
        _crawl_progress['retried'] = 0
        _crawl_progress['start_time'] = datetime.now().isoformat()
        _crawl_progress['end_time'] = None
    
    # è®°å½•é˜²åçˆ¬çŠ¶æ€
    if ANTI_SCRAPING_ENABLED:
        logger.info(f"ğŸ›¡ï¸ é˜²åçˆ¬å·²å¯ç”¨: UAè½®æ¢, éšèº«æ¨¡å¼, éšæœºå»¶è¿Ÿ({ANTI_SCRAPING_MIN_DELAY}-{ANTI_SCRAPING_MAX_DELAY}ç§’)")
        # é‡ç½®é˜²åçˆ¬ç®¡ç†å™¨ï¼Œç¡®ä¿æ¯æ¬¡çˆ¬å–ä½¿ç”¨æ–°çš„é…ç½®
        reset_anti_scraping_manager()
    
    logger.info(f"å¼€å§‹çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ï¼ˆå¹¶å‘æ•°: {CRAWL_CONCURRENCY}, æœ€å¤§é‡è¯•: {CRAWL_MAX_RETRIES}ï¼‰")
    start_time = datetime.now()
    
    # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore = asyncio.Semaphore(CRAWL_CONCURRENCY)
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨ç‹¬ç«‹æµè§ˆå™¨å®ä¾‹ï¼ˆæ›´ç¨³å®šï¼Œé¿å…å¹¶å‘å†²çªï¼‰
    use_shared_crawler = False
    
    shared_crawler = None
    if use_shared_crawler:
        try:
            shared_crawler = await create_shared_crawler()
            logger.info("ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œæå‡æ€§èƒ½")
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºå…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œä½¿ç”¨ç‹¬ç«‹å®ä¾‹: {e}")
            shared_crawler = None
    
    # ç¬¬ä¸€è½®çˆ¬å–ï¼šåªå°è¯•ä¸€æ¬¡ï¼Œä¸é‡è¯•ï¼ˆå¿«é€Ÿå¤±è´¥ï¼Œæ”¶é›†éœ€è¦é‡è¯•çš„æ–‡ç« ï¼‰
    failed_articles_lock = threading.Lock()  # ä¿æŠ¤å¤±è´¥æ–‡ç« åˆ—è¡¨çš„é”
    failed_articles = []
    
    async def crawl_with_progress(article: dict, index: int):
        """å¸¦è¿›åº¦æ›´æ–°çš„çˆ¬å–ä»»åŠ¡ï¼ˆç¬¬ä¸€è½®ï¼šåªå°è¯•ä¸€æ¬¡ï¼‰"""
        with _stop_signal_lock:
            if _stop_signal:
                return False
            
        try:
            # ç¬¬ä¸€è½®ï¼šåªå°è¯•ä¸€æ¬¡ï¼Œä¸é‡è¯•ï¼ˆskip_retry=Trueï¼‰
            result = await crawl_article_with_retry(
                article, 
                crawler=shared_crawler, 
                semaphore=semaphore,
                max_retries=CRAWL_MAX_RETRIES,
                skip_retry=True  # ç¬¬ä¸€è½®ä¸é‡è¯•
            )
            
            # æ›´æ–°è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            with _crawl_progress_lock:
                _crawl_progress['current'] = index + 1
                _crawl_progress['current_url'] = article['url']
                
                if result:
                    _crawl_progress['success'] += 1
                else:
                    _crawl_progress['failed'] += 1
            
            # æ”¶é›†å¤±è´¥çš„æ–‡ç« ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            if not result:
                with failed_articles_lock:
                    failed_articles.append(article)
            
            # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆé¿å…è¿‡äºé¢‘ç¹ï¼‰
            # æ³¨æ„ï¼šå¦‚æœå¯ç”¨äº†é˜²åçˆ¬éšæœºå»¶è¿Ÿï¼Œåˆ™ç”± extractors æ¨¡å—åœ¨æ¯æ¬¡çˆ¬å–æ—¶å¤„ç†
            # è¿™é‡Œåªåœ¨æœªå¯ç”¨é˜²åçˆ¬æ—¶ä½¿ç”¨å›ºå®šå»¶è¿Ÿ
            # ä¼˜åŒ–ï¼šå¦‚æœçˆ¬å–å¤±è´¥ï¼Œä¸å»¶è¿Ÿï¼ˆå¿«é€Ÿå¤±è´¥ï¼‰
            if not ANTI_SCRAPING_ENABLED and CRAWL_DELAY > 0 and result:
                await asyncio.sleep(CRAWL_DELAY)
            
            return result
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {article['url']}: {e}")
            # æ”¶é›†å¤±è´¥çš„æ–‡ç« ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            with failed_articles_lock:
                failed_articles.append(article)
            with _crawl_progress_lock:
                _crawl_progress['failed'] += 1
            return False
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡ï¼ˆç¬¬ä¸€è½®ï¼‰
    tasks = [crawl_with_progress(article, i) for i, article in enumerate(articles)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸ç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {articles[i]['url']}: {result}")
            with failed_articles_lock:
                if articles[i] not in failed_articles:
                    failed_articles.append(articles[i])
            with _crawl_progress_lock:
                _crawl_progress['failed'] += 1
    
    # æ¸…ç†å…±äº«æµè§ˆå™¨å®ä¾‹ï¼ˆç¬¬ä¸€è½®ç»“æŸï¼‰
    if shared_crawler:
        try:
            await shared_crawler.__aexit__(None, None, None)
        except Exception as e:
            logger.debug(f"æ¸…ç†å…±äº«æµè§ˆå™¨å®ä¾‹æ—¶å‡ºé”™: {e}")
    
    # ç¬¬äºŒè½®ï¼šé›†ä¸­é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ–‡ç« ï¼ˆå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼‰
    with _stop_signal_lock:
        should_retry = not _stop_signal and len(failed_articles) > 0
    
    if should_retry:
        logger.info(f"ğŸ”„ å¼€å§‹é›†ä¸­é‡è¯• {len(failed_articles)} ç¯‡å¤±è´¥æ–‡ç« ï¼ˆå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼‰")
        
        # ä½¿ç”¨æµè§ˆå™¨æ± è¿›è¡Œé›†ä¸­é‡è¯•
        from .browser_pool import get_browser_pool
        browser_pool = get_browser_pool()
        
        # æ‰¹é‡é‡è¯•ï¼ˆä½¿ç”¨æµè§ˆå™¨æ± ï¼‰
        retry_semaphore = asyncio.Semaphore(CRAWL_CONCURRENCY)
        retry_start_time = datetime.now()
        
        async def retry_with_pool(article: dict, index: int):
            """ä½¿ç”¨æµè§ˆå™¨æ± é‡è¯•å¤±è´¥çš„æ–‡ç« """
            with _stop_signal_lock:
                if _stop_signal:
                    return False
            
            try:
                # ä»æµè§ˆå™¨æ± è·å–å®ä¾‹
                crawler = await browser_pool.acquire()
                use_pool = True
                
                if not crawler:
                    # å¦‚æœæ± å·²æ»¡ï¼Œåˆ›å»ºç‹¬ç«‹å®ä¾‹
                    crawler = await create_shared_crawler()
                    use_pool = False
                
                try:
                    # æ‰§è¡Œé‡è¯•ï¼ˆè¿™æ¬¡å…è®¸é‡è¯•ï¼Œå¹¶æ ‡è®°é”™è¯¯ï¼‰
                    result = await _crawl_with_retry(article, crawler, CRAWL_MAX_RETRIES, mark_error_on_fail=True)
                    
                    # æ›´æ–°è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                    with _crawl_progress_lock:
                        _crawl_progress['current'] = len(articles) + index + 1
                        _crawl_progress['current_url'] = article['url']
                        
                        if result:
                            _crawl_progress['success'] += 1
                            _crawl_progress['failed'] -= 1  # ä»å¤±è´¥æ•°ä¸­å‡å»
                            _crawl_progress['retried'] += 1
                        else:
                            _crawl_progress['retried'] += 1
                    
                    # é‡è¯•åä»ç„¶å¤±è´¥ï¼Œç¡®ä¿æ ‡è®°ä¸º ERRORï¼ˆåŒé‡ä¿é™©ï¼‰
                    if not result:
                        update_article_status(article['id'], 'ERROR', 'é›†ä¸­é‡è¯•åä»ç„¶å¤±è´¥')
                    
                    return result
                finally:
                    if use_pool:
                        await browser_pool.release(crawler)
                    else:
                        await crawler.__aexit__(None, None, None)
            except Exception as e:
                logger.error(f"é‡è¯•å¼‚å¸¸ {article['url']}: {e}")
                # é‡è¯•å¼‚å¸¸æ—¶ä¹Ÿæ ‡è®°ä¸º ERROR
                update_article_status(article['id'], 'ERROR', str(e))
                with _crawl_progress_lock:
                    _crawl_progress['retried'] += 1
                return False
        
        # å¹¶å‘é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ–‡ç« 
        retry_tasks = []
        for i, article in enumerate(failed_articles):
            async def retry_with_semaphore(article, index):
                async with retry_semaphore:
                    return await retry_with_pool(article, index)
            retry_tasks.append(retry_with_semaphore(article, i))
        
        retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)
        
        retry_elapsed = (datetime.now() - retry_start_time).total_seconds()
        retry_success = sum(1 for r in retry_results if r is True)
        logger.info(f"ğŸ”„ é›†ä¸­é‡è¯•å®Œæˆ: {retry_success}/{len(failed_articles)} æˆåŠŸ, è€—æ—¶ {retry_elapsed:.2f} ç§’")
    
    # å®Œæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    with _crawl_progress_lock:
        _crawl_progress['is_running'] = False
        _crawl_progress['end_time'] = end_time.isoformat()
        _crawl_progress['current_url'] = None
        success_count = _crawl_progress['success']
        failed_count = _crawl_progress['failed']
        retried_count = _crawl_progress['retried']
    
    success_rate = (success_count / len(articles) * 100) if articles else 0
    logger.info(f"çˆ¬å–å®Œæˆ: {success_count}/{len(articles)} æˆåŠŸ ({success_rate:.1f}%), "
                f"{failed_count} å¤±è´¥, {retried_count} æ¬¡é‡è¯•, "
                f"è€—æ—¶ {elapsed:.2f} ç§’")
    if elapsed > 0:
        logger.info(f"å¹³å‡é€Ÿåº¦: {len(articles) / elapsed:.2f} æ–‡ç« /ç§’")

def crawl_all_sync():
    """åŒæ­¥åŒ…è£…å™¨"""
    try:
        asyncio.run(crawl_all_articles())
    except Exception as e:
        with _crawl_progress_lock:
            _crawl_progress['is_running'] = False
            _crawl_progress['end_time'] = datetime.now().isoformat()
        logger.error(f"çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
