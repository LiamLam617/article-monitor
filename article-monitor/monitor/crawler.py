"""
çˆ¬å–ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å–ï¼Œå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œé‡è¯•æœºåˆ¶ï¼Œé˜²åçˆ¬
"""
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
from .database import (
    get_all_articles, add_read_count, get_latest_read_count, 
    update_article_title, update_article_status
)
from .extractors import extract_read_count, extract_article_info, create_shared_crawler
from urllib.parse import urlparse
from .config import (
    SUPPORTED_SITES, CRAWL_CONCURRENCY, CRAWL_DELAY, 
    CRAWL_MAX_RETRIES, CRAWL_RETRY_DELAY, CRAWL_RETRY_BACKOFF,
    ANTI_SCRAPING_ENABLED, ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_MIN_DELAY, ANTI_SCRAPING_MAX_DELAY
)
from .anti_scraping import get_anti_scraping_manager, reset_anti_scraping_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€è¿›åº¦çŠ¶æ€
_crawl_progress = {
    'is_running': False,
    'total': 0,
    'current': 0,
    'success': 0,
    'failed': 0,
    'retried': 0,
    'current_url': None,
    'start_time': None,
    'end_time': None
}

# å…¨å±€åœæ­¢ä¿¡å·
_stop_signal = False

def stop_crawling():
    """åœæ­¢çˆ¬å–ä»»åŠ¡"""
    global _stop_signal
    _stop_signal = True
    logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢çˆ¬å–...")

def get_crawl_progress():
    """è·å–çˆ¬å–è¿›åº¦"""
    return _crawl_progress.copy()

def reset_crawl_progress():
    """é‡ç½®çˆ¬å–è¿›åº¦"""
    global _crawl_progress
    _crawl_progress = {
        'is_running': False,
        'total': 0,
        'current': 0,
        'success': 0,
        'failed': 0,
        'retried': 0,
        'current_url': None,
        'start_time': None,
        'end_time': None
    }

def _is_retryable_error(error: Exception) -> bool:
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•"""
    error_str = str(error).lower()
    retryable_keywords = [
        'timeout', 'connection', 'network', 'temporary',
        '503', '502', '504', '429',  # HTTPé”™è¯¯ç 
        'econnrefused', 'econnreset', 'etimedout',
        'ssl', 'certificate', 'handshake'
    ]
    return any(keyword in error_str for keyword in retryable_keywords)

async def crawl_article_with_retry(article: dict, crawler=None, semaphore=None, max_retries: int = None) -> bool:
    """çˆ¬å–å•ç¯‡æ–‡ç« ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        semaphore: å¯é€‰çš„å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
    """
    if max_retries is None:
        max_retries = CRAWL_MAX_RETRIES
    
    url = article['url']
    article_id = article['id']
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    if semaphore:
        async with semaphore:
            if _stop_signal:
                return False
            return await _crawl_with_retry(article, crawler, max_retries)
    else:
        if _stop_signal:
            return False
        return await _crawl_with_retry(article, crawler, max_retries)

async def _crawl_with_retry(article: dict, crawler=None, max_retries: int = 3) -> bool:
    """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–é€»è¾‘ï¼ˆåŒæ—¶æ›´æ–°æ ‡é¢˜ï¼‰"""
    url = article['url']
    article_id = article['id']
    current_title = article.get('title', '')
    
    last_error = None
    
    for attempt in range(max_retries + 1):  # 0åˆ°max_retriesï¼Œå…±max_retries+1æ¬¡å°è¯•
        try:
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt > 0:
                # æŒ‡æ•°é€€é¿ï¼šå»¶è¿Ÿæ—¶é—´ = åŸºç¡€å»¶è¿Ÿ * (é€€é¿å€æ•° ^ å°è¯•æ¬¡æ•°)
                delay = CRAWL_RETRY_DELAY * (CRAWL_RETRY_BACKOFF ** (attempt - 1))
                logger.info(f"ğŸ”„ é‡è¯• {attempt}/{max_retries}: {url} (ç­‰å¾… {delay:.1f}ç§’)")
                await asyncio.sleep(delay)
                
                # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨ç¡çœ æœŸé—´å¯èƒ½æ”¶åˆ°äº†åœæ­¢ä¿¡å·ï¼‰
                if _stop_signal:
                    logger.info(f"ğŸ›‘ ä»»åŠ¡å·²åœæ­¢: {url}")
                    return False
                    
                global _crawl_progress
                _crawl_progress['retried'] += 1
            
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
            if _stop_signal:
                return False
                
            # æ‰§è¡Œçˆ¬å–ï¼ˆåŒæ—¶è·å–é˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
            info = await extract_article_info(url, crawler)
            count = info.get('read_count')
            new_title = info.get('title')
            
            # æ›´æ–°æ–‡ç« æ ‡é¢˜ï¼ˆå¦‚æœæœ‰æ–°æ ‡é¢˜ä¸”ä¸å½“å‰æ ‡é¢˜ä¸åŒï¼‰
            if new_title and new_title != current_title:
                if update_article_title(article_id, new_title):
                    logger.info(f"ğŸ“ æ›´æ–°æ ‡é¢˜: {new_title[:30]}...")
            
            if count is None:
                # å¦‚æœæå–å¤±è´¥ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                if attempt < max_retries:
                    logger.debug(f"âš ï¸  æå–å¤±è´¥ï¼Œå°†é‡è¯•: {url} (å°è¯• {attempt + 1}/{max_retries + 1})")
                    continue
                else:
                    logger.warning(f"âŒ æ— æ³•æå–é˜…è¯»æ•°: {url} (å·²é‡è¯• {max_retries} æ¬¡)")
                    return False
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆé¿å…é‡å¤ç›¸åŒæ•°æ®ï¼‰
            latest = get_latest_read_count(article_id)
            if latest and latest['count'] == count:
                logger.debug(f"âœ“ é˜…è¯»æ•°æœªå˜åŒ–: {url} ({count})")
                return True
            
            # ä¿å­˜é˜…è¯»æ•°
            add_read_count(article_id, count)
            
            # æ›´æ–°çŠ¶æ€ä¸ºæˆåŠŸ
            update_article_status(article_id, 'OK')
            
            if attempt > 0:
                logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url} -> {count} (å°è¯• {attempt + 1} æ¬¡)")
            else:
                logger.info(f"âœ… æ›´æ–°æˆåŠŸ: {url} -> {count}")
            return True
            
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•
            is_retryable = _is_retryable_error(e)
            
            if is_retryable and attempt < max_retries:
                logger.warning(f"âš ï¸  å¯é‡è¯•é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {url} - {str(e)[:100]}")
                continue
            else:
                # ä¸å¯é‡è¯•æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                error_msg = str(e)[:100]
                if 'timeout' in error_str or 'connection' in error_str:
                    logger.error(f"â±ï¸  ç½‘ç»œé”™è¯¯ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                else:
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆå¦‚æœåœ¨æœ€ç»ˆå¤±è´¥å‰è®°å½•ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªåœ¨æœ€åä¸€æ¬¡å°è¯•å¤±è´¥åæ‰æ ‡è®°ä¸ºERRORï¼Œæˆ–è€…ä¸å¯é‡è¯•é”™è¯¯æ—¶
                if not is_retryable or attempt >= max_retries:
                    update_article_status(article_id, 'ERROR', str(e))
                
                return False
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    final_error = str(last_error) if last_error else 'æœªçŸ¥é”™è¯¯'
    logger.error(f"âŒ çˆ¬å–æœ€ç»ˆå¤±è´¥ {url}: {final_error[:100]}")
    update_article_status(article_id, 'ERROR', final_error)
    return False

async def crawl_article(article: dict, crawler=None, semaphore=None) -> bool:
    """çˆ¬å–å•ç¯‡æ–‡ç« ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        semaphore: å¯é€‰çš„å¹¶å‘æ§åˆ¶ä¿¡å·é‡
    """
    return await crawl_article_with_retry(article, crawler, semaphore)

async def crawl_all_articles():
    """çˆ¬å–æ‰€æœ‰æ–‡ç«  - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å– + é‡è¯•æœºåˆ¶"""
    global _crawl_progress, _stop_signal
    
    # é‡ç½®åœæ­¢ä¿¡å·
    _stop_signal = False
    
    articles = get_all_articles()
    if not articles:
        logger.info("æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ–‡ç« ")
        reset_crawl_progress()
        return
    
    # åˆå§‹åŒ–è¿›åº¦
    _crawl_progress['is_running'] = True
    _crawl_progress['total'] = len(articles)
    _crawl_progress['current'] = 0
    _crawl_progress['success'] = 0
    _crawl_progress['failed'] = 0
    _crawl_progress['retried'] = 0
    _crawl_progress['start_time'] = datetime.now().isoformat()
    _crawl_progress['end_time'] = None
    
    # è®°å½•é˜²åçˆ¬çŠ¶æ€
    if ANTI_SCRAPING_ENABLED:
        logger.info(f"ğŸ›¡ï¸ é˜²åçˆ¬å·²å¯ç”¨: UAè½®æ¢, éšèº«æ¨¡å¼, éšæœºå»¶è¿Ÿ({ANTI_SCRAPING_MIN_DELAY}-{ANTI_SCRAPING_MAX_DELAY}ç§’)")
        # é‡ç½®é˜²åçˆ¬ç®¡ç†å™¨ï¼Œç¡®ä¿æ¯æ¬¡çˆ¬å–ä½¿ç”¨æ–°çš„é…ç½®
        reset_anti_scraping_manager()
    
    logger.info(f"å¼€å§‹çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ï¼ˆå¹¶å‘æ•°: {CRAWL_CONCURRENCY}, æœ€å¤§é‡è¯•: {CRAWL_MAX_RETRIES}ï¼‰")
    start_time = datetime.now()
    
    # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore = asyncio.Semaphore(CRAWL_CONCURRENCY)
    
    # æ³¨æ„ï¼šå…±äº«æµè§ˆå™¨å®ä¾‹åœ¨å¹¶å‘åœºæ™¯ä¸‹å¯èƒ½æœ‰é—®é¢˜
    # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹å®ä¾‹æ›´ç¨³å®šï¼Œä½†æ€§èƒ½ç¨å·®
    # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©
    use_shared_crawler = False  # æš‚æ—¶ç¦ç”¨å…±äº«å®ä¾‹ï¼Œæé«˜ç¨³å®šæ€§
    
    shared_crawler = None
    if use_shared_crawler:
        try:
            shared_crawler = await create_shared_crawler()
            logger.info("ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œæå‡æ€§èƒ½")
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºå…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œä½¿ç”¨ç‹¬ç«‹å®ä¾‹: {e}")
            shared_crawler = None
    
    # åˆ›å»ºçˆ¬å–ä»»åŠ¡åˆ—è¡¨
    async def crawl_with_progress(article: dict, index: int):
        """å¸¦è¿›åº¦æ›´æ–°çš„çˆ¬å–ä»»åŠ¡"""
        if _stop_signal:
            return False
            
        try:
            result = await crawl_article_with_retry(
                article, 
                crawler=shared_crawler, 
                semaphore=semaphore,
                max_retries=CRAWL_MAX_RETRIES
            )
            
            # æ›´æ–°è¿›åº¦
            _crawl_progress['current'] = index + 1
            _crawl_progress['current_url'] = article['url']
            
            if result:
                _crawl_progress['success'] += 1
            else:
                _crawl_progress['failed'] += 1
            
            # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆé¿å…è¿‡äºé¢‘ç¹ï¼‰
            # å¦‚æœå¯ç”¨äº†é˜²åçˆ¬éšæœºå»¶è¿Ÿï¼Œåˆ™ç”± extractors æ¨¡å—å¤„ç†
            # è¿™é‡Œåªåœ¨æœªå¯ç”¨é˜²åçˆ¬æ—¶ä½¿ç”¨å›ºå®šå»¶è¿Ÿ
            if not ANTI_SCRAPING_ENABLED and CRAWL_DELAY > 0:
                await asyncio.sleep(CRAWL_DELAY)
            
            return result
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {article['url']}: {e}")
            _crawl_progress['failed'] += 1
            return False
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡
    tasks = [crawl_with_progress(article, i) for i, article in enumerate(articles)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸ç»“æœ
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {articles[i]['url']}: {result}")
            _crawl_progress['failed'] += 1
    
    # æ¸…ç†å…±äº«æµè§ˆå™¨å®ä¾‹
    if shared_crawler:
        try:
            await shared_crawler.__aexit__(None, None, None)
        except:
            pass
    
    # å®Œæˆ
    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    _crawl_progress['is_running'] = False
    _crawl_progress['end_time'] = end_time.isoformat()
    _crawl_progress['current_url'] = None
    
    success_rate = (_crawl_progress['success'] / len(articles) * 100) if articles else 0
    logger.info(f"çˆ¬å–å®Œæˆ: {_crawl_progress['success']}/{len(articles)} æˆåŠŸ ({success_rate:.1f}%), "
                f"{_crawl_progress['failed']} å¤±è´¥, {_crawl_progress['retried']} æ¬¡é‡è¯•, "
                f"è€—æ—¶ {elapsed:.2f} ç§’")
    if elapsed > 0:
        logger.info(f"å¹³å‡é€Ÿåº¦: {len(articles) / elapsed:.2f} æ–‡ç« /ç§’")

def crawl_all_sync():
    """åŒæ­¥åŒ…è£…å™¨"""
    try:
        asyncio.run(crawl_all_articles())
    except Exception as e:
        global _crawl_progress
        _crawl_progress['is_running'] = False
        _crawl_progress['end_time'] = datetime.now().isoformat()
        logger.error(f"çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
