"""
çˆ¬å–ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å–ï¼Œå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œé‡è¯•æœºåˆ¶ï¼Œé˜²åçˆ¬
æ”¯æŒæ¯åŸŸåå¹¶å‘é™åˆ¶ä¸æŒ‰ç«™ç‚¹äº¤é”™è°ƒåº¦ï¼Œæé«˜åŒä¸€ç«™ç‚¹å¤§é‡æ–‡ç« æ—¶çš„æˆåŠŸç‡
"""
import asyncio
import threading
import time
import logging
import random
from collections import defaultdict
from datetime import datetime
from typing import List, Dict, Optional
from enum import Enum
from .database import (
    get_all_articles, add_read_count, get_latest_read_count,
    get_latest_read_counts_batch, update_article_title, update_article_status
)
from .extractors import extract_article_info, create_shared_crawler
from urllib.parse import urlparse
from .config import (
    SUPPORTED_SITES, CRAWL_CONCURRENCY, CRAWL_DELAY,
    CRAWL_CONCURRENCY_PER_DOMAIN, CRAWL_INTERLEAVE_BY_SITE, CRAWL_MIN_DELAY_PER_DOMAIN,
    CRAWL_MAX_RETRIES, CRAWL_RETRY_DELAY, CRAWL_RETRY_BACKOFF,
    CRAWL_RETRY_MAX_DELAY, CRAWL_RETRY_JITTER,
    CRAWL_RETRY_NETWORK_MAX, CRAWL_RETRY_PARSE_MAX, CRAWL_RETRY_SSL_MAX, CRAWL_RETRY_SSL_DELAY,
    ANTI_SCRAPING_ENABLED, ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_MIN_DELAY, ANTI_SCRAPING_MAX_DELAY,
    is_platform_allowed
)
from .anti_scraping import get_anti_scraping_manager, reset_anti_scraping_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€è¿›åº¦çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤ï¼‰
_crawl_progress_lock = threading.Lock()
_crawl_progress = {
    'is_running': False,
    'total': 0,
    'current': 0,
    'success': 0,
    'failed': 0,
    'retried': 0,
    'current_url': None,
    'start_time': None,
    'end_time': None
}

# å…¨å±€åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤ï¼‰
_stop_signal_lock = threading.Lock()
_stop_signal = False

def stop_crawling():
    """åœæ­¢çˆ¬å–ä»»åŠ¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _stop_signal
    with _stop_signal_lock:
        _stop_signal = True
    logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢çˆ¬å–...")

def get_crawl_progress():
    """è·å–çˆ¬å–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with _crawl_progress_lock:
        return _crawl_progress.copy()

def reset_crawl_progress():
    """é‡ç½®çˆ¬å–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _crawl_progress
    with _crawl_progress_lock:
        _crawl_progress = {
            'is_running': False,
            'total': 0,
            'current': 0,
            'success': 0,
            'failed': 0,
            'retried': 0,
            'current_url': None,
            'start_time': None,
            'end_time': None
        }


def _domain_from_article(article: dict) -> str:
    """ä»æ–‡ç«  URL æå– netloc ä½œä¸ºåŸŸå keyï¼ˆç”¨äºæ¯åŸŸåé™æµï¼‰ã€‚"""
    url = article.get('url') or ''
    try:
        return urlparse(url).netloc.lower() or 'unknown'
    except Exception:
        return 'unknown'


# æ¯åŸŸåä¿¡å·é‡æ³¨å†Œè¡¨ï¼ˆlazy åˆ›å»ºï¼Œä»…åœ¨å•æ¬¡ crawl è¿è¡Œå†…ä½¿ç”¨ï¼Œkey=domain, value=Semaphoreï¼‰
_domain_semaphores: Dict[str, asyncio.Semaphore] = {}
# æ¯äº‹ä»¶å¾ªç¯ä¸€ä¸ª asyncio.Lockï¼Œé¿å…ã€Œbound to a different event loopã€ï¼ˆçˆ¬å–åœ¨å­çº¿ç¨‹çš„ loop ä¸­è¿è¡Œï¼‰
_loop_locks_guard = threading.Lock()
_domain_semaphores_lock_by_loop: Dict[asyncio.AbstractEventLoop, asyncio.Lock] = {}
_domain_rate_limit_lock_by_loop: Dict[asyncio.AbstractEventLoop, asyncio.Lock] = {}


def _get_semaphores_lock_for_current_loop() -> asyncio.Lock:
    """è¿”å›å½“å‰äº‹ä»¶å¾ªç¯å¯¹åº”çš„ã€Œä¿¡å·é‡è¡¨ã€é”ï¼ˆæ‡’åˆ›å»ºï¼Œçº¿ç¨‹å®‰å…¨ï¼‰ã€‚"""
    loop = asyncio.get_running_loop()
    with _loop_locks_guard:
        for closed_loop in [loop for loop in _domain_semaphores_lock_by_loop if loop.is_closed()]:
            _domain_semaphores_lock_by_loop.pop(closed_loop, None)
        if loop not in _domain_semaphores_lock_by_loop:
            _domain_semaphores_lock_by_loop[loop] = asyncio.Lock()
        return _domain_semaphores_lock_by_loop[loop]


def _get_rate_limit_lock_for_current_loop() -> asyncio.Lock:
    """è¿”å›å½“å‰äº‹ä»¶å¾ªç¯å¯¹åº”çš„ã€Œæ¯åŸŸåé™é€Ÿã€é”ï¼ˆæ‡’åˆ›å»ºï¼Œçº¿ç¨‹å®‰å…¨ï¼‰ã€‚"""
    loop = asyncio.get_running_loop()
    with _loop_locks_guard:
        for closed_loop in [loop for loop in _domain_rate_limit_lock_by_loop if loop.is_closed()]:
            _domain_rate_limit_lock_by_loop.pop(closed_loop, None)
        if loop not in _domain_rate_limit_lock_by_loop:
            _domain_rate_limit_lock_by_loop[loop] = asyncio.Lock()
        return _domain_rate_limit_lock_by_loop[loop]


async def _get_domain_semaphore(domain: str) -> Optional[asyncio.Semaphore]:
    """è¿”å›è¯¥åŸŸåçš„ä¿¡å·é‡ï¼›è‹¥ CRAWL_CONCURRENCY_PER_DOMAIN ä¸º 0 åˆ™è¿”å› Noneï¼ˆä¸é™åˆ¶ï¼‰ã€‚"""
    if CRAWL_CONCURRENCY_PER_DOMAIN <= 0:
        return None
    sem_lock = _get_semaphores_lock_for_current_loop()
    async with sem_lock:
        if domain not in _domain_semaphores:
            _domain_semaphores[domain] = asyncio.Semaphore(CRAWL_CONCURRENCY_PER_DOMAIN)
        return _domain_semaphores[domain]


def _reset_domain_semaphores():
    """æ¸…ç©ºæ¯åŸŸåä¿¡å·é‡è¡¨ï¼ˆåœ¨æ¯æ¬¡ crawl å¼€å§‹æ—¶è°ƒç”¨ï¼Œé¿å…è·¨æ¬¡ç´¯ç§¯ï¼‰ã€‚"""
    global _domain_semaphores
    _domain_semaphores = {}
    with _loop_locks_guard:
        for closed_loop in [loop for loop in _domain_semaphores_lock_by_loop if loop.is_closed()]:
            _domain_semaphores_lock_by_loop.pop(closed_loop, None)


# æ¯åŸŸåä¸Šæ¬¡è¯·æ±‚ç»“æŸæ—¶é—´ï¼ˆç”¨äº CRAWL_MIN_DELAY_PER_DOMAINï¼‰
_domain_last_request_time: Dict[str, float] = {}


async def _wait_domain_rate_limit(domain: str) -> None:
    """è‹¥é…ç½®äº† CRAWL_MIN_DELAY_PER_DOMAINï¼Œåˆ™ç­‰å¾…è‡³æ»¡è¶³æœ€å°é—´éš”ã€‚"""
    if CRAWL_MIN_DELAY_PER_DOMAIN <= 0:
        return
    now = time.monotonic()
    rate_limit_lock = _get_rate_limit_lock_for_current_loop()
    async with rate_limit_lock:
        last = _domain_last_request_time.get(domain, 0)
        sleep_time = last + CRAWL_MIN_DELAY_PER_DOMAIN - now
    if sleep_time > 0:
        await asyncio.sleep(sleep_time)


async def _record_domain_request_done(domain: str) -> None:
    """è®°å½•è¯¥åŸŸåæœ¬æ¬¡è¯·æ±‚å·²ç»“æŸï¼ˆåœ¨ crawl å®Œæˆåè°ƒç”¨ï¼‰ã€‚"""
    if CRAWL_MIN_DELAY_PER_DOMAIN <= 0:
        return
    rate_limit_lock = _get_rate_limit_lock_for_current_loop()
    async with rate_limit_lock:
        _domain_last_request_time[domain] = time.monotonic()


def _reset_domain_rate_limit():
    """æ¸…ç©ºæ¯åŸŸåè¯·æ±‚æ—¶é—´ï¼ˆæ¯æ¬¡ crawl å¼€å§‹æ—¶è°ƒç”¨ï¼‰ã€‚"""
    global _domain_last_request_time
    _domain_last_request_time = {}
    with _loop_locks_guard:
        for closed_loop in [loop for loop in _domain_rate_limit_lock_by_loop if loop.is_closed()]:
            _domain_rate_limit_lock_by_loop.pop(closed_loop, None)


def _interleave_articles_by_site(articles: List[dict]) -> List[dict]:
    """æŒ‰ç«™ç‚¹ round-robin æ‰“æ•£æ–‡ç« é¡ºåºï¼Œä½¿å¹¶å‘æ›´å‡åŒ€åˆ†å¸ƒåœ¨å¤šç«™ç‚¹ã€‚"""
    if not articles:
        return articles
    groups = defaultdict(list)
    for a in articles:
        key = _domain_from_article(a)
        groups[key].append(a)
    keys = list(groups.keys())
    interleaved = []
    idx = 0
    while any(groups[k] for k in keys):
        k = keys[idx % len(keys)]
        if groups[k]:
            interleaved.append(groups[k].pop(0))
        idx += 1
    return interleaved


class ErrorCategory(Enum):
    """é”™è¯¯åˆ†ç±»æšä¸¾"""
    NETWORK = 'network'  # ç½‘ç»œé”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
    PARSE = 'parse'  # è§£æé”™è¯¯ï¼ˆå¯é‡è¯•ï¼Œä½†é‡è¯•æ¬¡æ•°è¾ƒå°‘ï¼‰
    SSL = 'ssl'  # SSL/è¯ä¹¦é”™è¯¯ï¼ˆå¯é‡è¯•ï¼Œéœ€è¦æ›´é•¿å»¶è¿Ÿï¼‰
    PERMANENT = 'permanent'  # æ°¸ä¹…æ€§é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰
    UNKNOWN = 'unknown'  # æœªçŸ¥é”™è¯¯ï¼ˆé»˜è®¤å¯é‡è¯•ï¼‰

# é‡è¯•ä¼˜å…ˆçº§æ˜ å°„ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
RETRY_PRIORITY_MAP = {
    ErrorCategory.NETWORK: 1,
    ErrorCategory.UNKNOWN: 1,
    ErrorCategory.PARSE: 2,
    ErrorCategory.SSL: 3,
    ErrorCategory.PERMANENT: 4
}

def _get_error_category(error: Exception) -> ErrorCategory:
    """æ™ºèƒ½é”™è¯¯åˆ†ç±»
    
    Args:
        error: å¼‚å¸¸å¯¹è±¡
        
    Returns:
        ErrorCategory: é”™è¯¯åˆ†ç±»
    """
    error_str = str(error).lower()
    
    # æ°¸ä¹…æ€§é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰
    permanent_keywords = ['404', 'not found', '403', 'forbidden', '401', 'unauthorized']
    if any(keyword in error_str for keyword in permanent_keywords):
        return ErrorCategory.PERMANENT
    
    # SSL/è¯ä¹¦é”™è¯¯
    ssl_keywords = ['ssl', 'certificate', 'handshake', 'tls', 'cert']
    if any(keyword in error_str for keyword in ssl_keywords):
        return ErrorCategory.SSL
    
    # ç½‘ç»œé”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
    network_keywords = [
        'timeout', 'connection', 'network', 'temporary',
        '503', '502', '504', '429',  # HTTPé”™è¯¯ç 
        'econnrefused', 'econnreset', 'etimedout',
        'connection refused', 'connection reset', 'connection aborted',
        'name resolution', 'dns', 'no route to host'
    ]
    if any(keyword in error_str for keyword in network_keywords):
        return ErrorCategory.NETWORK
    
    # è§£æé”™è¯¯ï¼ˆæå–å¤±è´¥ã€é¡µé¢ç»“æ„å˜åŒ–ï¼‰
    parse_keywords = ['extract', 'parse', 'selector', 'element not found', 'no such element']
    if any(keyword in error_str for keyword in parse_keywords):
        return ErrorCategory.PARSE
    
    # é»˜è®¤ï¼šæœªçŸ¥é”™è¯¯ï¼Œè§†ä¸ºç½‘ç»œé”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
    return ErrorCategory.UNKNOWN

def _is_retryable_error(error: Exception) -> bool:
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    category = _get_error_category(error)
    return category != ErrorCategory.PERMANENT

async def crawl_article_with_retry(article: dict, crawler=None, semaphore=None, max_retries: int = None, skip_retry: bool = False) -> bool:
    """çˆ¬å–å•ç¯‡æ–‡ç« ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        semaphore: å¯é€‰çš„å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        skip_retry: æ˜¯å¦è·³è¿‡é‡è¯•ï¼ˆç”¨äºç¬¬ä¸€è½®çˆ¬å–ï¼Œåªå°è¯•ä¸€æ¬¡ï¼‰
    """
    if max_retries is None:
        max_retries = CRAWL_MAX_RETRIES
    
    url = article['url']
    article_id = article['id']
    site = article.get('site')
    
    # æ£€æŸ¥å¹³å°æ˜¯å¦åœ¨ç™½åå•ä¸­ï¼ˆåŒé‡ä¿é™©ï¼‰
    if not is_platform_allowed(site):
        logger.info(f"â­ï¸  è·³è¿‡éç™½åå•å¹³å°æ–‡ç« : {url} (å¹³å°: {site})")
        return False
    
    # æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _stop_signal_lock:
        if _stop_signal:
            return False
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼ˆä¼˜åŒ–ï¼šæ¶ˆé™¤é‡å¤ä»£ç ï¼‰
    async def _do_crawl():
        return await _crawl_with_retry(
            article, crawler, 
            max_retries if not skip_retry else 0,
            mark_error_on_fail=not skip_retry
        )
    
    if semaphore:
        async with semaphore:
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨è·å–ä¿¡å·é‡åï¼‰
            with _stop_signal_lock:
                if _stop_signal:
                    return False
            return await _do_crawl()
    else:
        return await _do_crawl()

def _calculate_retry_delay(error_category: ErrorCategory, attempt: int) -> float:
    """è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæ ¹æ®é”™è¯¯ç±»å‹å’Œå°è¯•æ¬¡æ•°ï¼‰
    
    Args:
        error_category: é”™è¯¯åˆ†ç±»
        attempt: å½“å‰å°è¯•æ¬¡æ•°ï¼ˆä»1å¼€å§‹ï¼‰
        
    Returns:
        float: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    if error_category == ErrorCategory.SSL:
        # SSLé”™è¯¯ï¼šå›ºå®šé•¿å»¶è¿Ÿ
        base_delay = CRAWL_RETRY_SSL_DELAY
    elif error_category == ErrorCategory.PARSE:
        # è§£æé”™è¯¯ï¼šçº¿æ€§é€€é¿
        base_delay = CRAWL_RETRY_DELAY * attempt
    else:
        # ç½‘ç»œé”™è¯¯å’ŒæœªçŸ¥é”™è¯¯ï¼šæŒ‡æ•°é€€é¿
        base_delay = CRAWL_RETRY_DELAY * (CRAWL_RETRY_BACKOFF ** (attempt - 1))
    
    # åº”ç”¨æœ€å¤§å»¶è¿Ÿä¸Šé™
    delay = min(base_delay, CRAWL_RETRY_MAX_DELAY)
    
    # æ·»åŠ æŠ–åŠ¨ï¼ˆjitterï¼‰é¿å…é›·ç¾¤æ•ˆåº”
    if CRAWL_RETRY_JITTER:
        jitter = random.uniform(-0.1 * delay, 0.1 * delay)
        delay = max(0.1, delay + jitter)  # ç¡®ä¿å»¶è¿Ÿä¸ä¸ºè´Ÿ
    
    return delay

def _get_max_retries_for_category(error_category: ErrorCategory) -> int:
    """æ ¹æ®é”™è¯¯ç±»å‹è·å–æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Args:
        error_category: é”™è¯¯åˆ†ç±»
        
    Returns:
        int: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    if error_category == ErrorCategory.NETWORK or error_category == ErrorCategory.UNKNOWN:
        return CRAWL_RETRY_NETWORK_MAX
    elif error_category == ErrorCategory.PARSE:
        return CRAWL_RETRY_PARSE_MAX
    elif error_category == ErrorCategory.SSL:
        return CRAWL_RETRY_SSL_MAX
    else:  # PERMANENT
        return 0

async def _crawl_with_retry(article: dict, crawler=None, max_retries: int = 3, mark_error_on_fail: bool = True) -> bool:
    """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–é€»è¾‘ï¼ˆåŒæ—¶æ›´æ–°æ ‡é¢˜ï¼‰
    
    Args:
        article: æ–‡ç« ä¿¡æ¯å­—å…¸
        crawler: å¯é€‰çš„æµè§ˆå™¨å®ä¾‹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        mark_error_on_fail: å¤±è´¥æ—¶æ˜¯å¦ç«‹å³æ ‡è®°ä¸ºERRORï¼ˆç¬¬ä¸€è½®ä¸æ ‡è®°ï¼Œç¬¬äºŒè½®æ ‡è®°ï¼‰
    """
    url = article['url']
    article_id = article['id']
    current_title = article.get('title', '')
    
    last_error = None
    last_error_category = None
    
    for attempt in range(max_retries + 1):  # 0åˆ°max_retriesï¼Œå…±max_retries+1æ¬¡å°è¯•
        try:
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            if attempt > 0 and last_error_category:
                # æ ¹æ®é”™è¯¯ç±»å‹è®¡ç®—å»¶è¿Ÿ
                delay = _calculate_retry_delay(last_error_category, attempt)
                logger.info(f"ğŸ”„ é‡è¯• {attempt}/{max_retries}: {url} (é”™è¯¯ç±»å‹: {last_error_category.value}, ç­‰å¾… {delay:.1f}ç§’)")
                await asyncio.sleep(delay)
                
                # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·ï¼ˆåœ¨ç¡çœ æœŸé—´å¯èƒ½æ”¶åˆ°äº†åœæ­¢ä¿¡å·ï¼‰
                with _stop_signal_lock:
                    if _stop_signal:
                        logger.info(f"ğŸ›‘ ä»»åŠ¡å·²åœæ­¢: {url}")
                        return False
                    
                with _crawl_progress_lock:
                    _crawl_progress['retried'] += 1
            
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
            with _stop_signal_lock:
                if _stop_signal:
                    return False
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆåŒæ—¶è·å–é˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
            info = await extract_article_info(url, crawler)
            count = info.get('read_count')
            new_title = info.get('title')
            
            # æ›´æ–°æ–‡ç« æ ‡é¢˜ï¼ˆå¦‚æœæœ‰æ–°æ ‡é¢˜ä¸”ä¸å½“å‰æ ‡é¢˜ä¸åŒï¼‰
            if new_title and new_title != current_title:
                if update_article_title(article_id, new_title):
                    logger.info(f"ğŸ“ æ›´æ–°æ ‡é¢˜: {new_title[:30]}...")
            
            if count is None:
                # æå–å¤±è´¥è§†ä¸ºè§£æé”™è¯¯
                parse_error = Exception('æ— æ³•æå–é˜…è¯»æ•°')
                last_error_category = ErrorCategory.PARSE
                category_max_retries = _get_max_retries_for_category(last_error_category)
                effective_max_retries = min(max_retries, category_max_retries)
                
                if attempt <= effective_max_retries:
                    logger.info(f"âš ï¸  æå–å¤±è´¥ï¼ˆè§£æé”™è¯¯ï¼‰ï¼Œå°†é‡è¯•: {url} (å°è¯• {attempt + 1}/{effective_max_retries + 1})")
                    last_error = parse_error
                    continue
                else:
                    logger.warning(f"âŒ æ— æ³•æå–é˜…è¯»æ•°: {url} (å·²é‡è¯• {effective_max_retries} æ¬¡)")
                    # æå–å¤±è´¥æ—¶ä¸æ ‡è®°ERRORï¼ˆç­‰å¾…é›†ä¸­é‡è¯•ï¼‰
                    if mark_error_on_fail:
                        update_article_status(article_id, 'ERROR', 'æ— æ³•æå–é˜…è¯»æ•°')
                    return False
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆé¿å…é‡å¤ç›¸åŒæ•°æ®ï¼‰
            # ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„æœ€æ–°é˜…è¯»æ•°ï¼Œé¿å…æ•°æ®åº“æŸ¥è¯¢
            latest_count = article.get('_latest_count')
            if latest_count is None:
                # å¦‚æœé¢„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢
                latest = get_latest_read_count(article_id)
                latest_count = latest['count'] if latest else None
            
            if latest_count is not None and latest_count == count:
                logger.debug(f"âœ“ é˜…è¯»æ•°æœªå˜åŒ–: {url} ({count})")
                # å³ä½¿æœªå˜åŒ–ï¼Œä¹Ÿæ›´æ–°çŠ¶æ€ä¸ºæˆåŠŸï¼ˆè¡¨ç¤ºçˆ¬å–æ­£å¸¸ï¼‰
                update_article_status(article_id, 'OK')
                return True
            
            # ä¿å­˜é˜…è¯»æ•°
            add_read_count(article_id, count)
            
            # æ›´æ–°çŠ¶æ€ä¸ºæˆåŠŸ
            update_article_status(article_id, 'OK')
            
            if attempt > 0:
                logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url} -> {count} (å°è¯• {attempt + 1} æ¬¡)")
            else:
                logger.info(f"âœ… æ›´æ–°æˆåŠŸ: {url} -> {count}")
            return True
            
        except Exception as e:
            last_error = e
            last_error_category = _get_error_category(e)
            error_str = str(e).lower()
            
            # å¿«é€Ÿå¤±è´¥ï¼šæ°¸ä¹…æ€§é”™è¯¯ç«‹å³å¤±è´¥ï¼Œä¸é‡è¯•
            if last_error_category == ErrorCategory.PERMANENT:
                error_msg = str(e)[:100]
                logger.error(f"âŒ æ°¸ä¹…æ€§é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰{url}: {error_msg}")
                if mark_error_on_fail:
                    update_article_status(article_id, 'ERROR', error_msg)
                return False
            
            # æ ¹æ®é”™è¯¯ç±»å‹è·å–æœ€å¤§é‡è¯•æ¬¡æ•°
            category_max_retries = _get_max_retries_for_category(last_error_category)
            effective_max_retries = min(max_retries, category_max_retries)
            
            # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•
            is_retryable = _is_retryable_error(e)
            
            if is_retryable and attempt <= effective_max_retries:
                logger.warning(f"âš ï¸  å¯é‡è¯•é”™è¯¯ [{last_error_category.value}] (å°è¯• {attempt + 1}/{effective_max_retries + 1}): {url} - {str(e)[:100]}")
                continue
            else:
                # ä¸å¯é‡è¯•æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                error_msg = str(e)[:100]
                if last_error_category == ErrorCategory.NETWORK:
                    logger.error(f"â±ï¸  ç½‘ç»œé”™è¯¯ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                elif last_error_category == ErrorCategory.PARSE:
                    logger.error(f"ğŸ” è§£æé”™è¯¯ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                elif last_error_category == ErrorCategory.SSL:
                    logger.error(f"ğŸ”’ SSLé”™è¯¯ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                else:
                    logger.error(f"âŒ çˆ¬å–å¤±è´¥ {url}: {error_msg} (å·²é‡è¯• {attempt} æ¬¡)")
                
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆæ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ ‡è®°ï¼‰
                # ç¬¬ä¸€è½®ä¸æ ‡è®°ERRORï¼ˆç­‰å¾…é›†ä¸­é‡è¯•ï¼‰ï¼Œç¬¬äºŒè½®æ‰æ ‡è®°
                if mark_error_on_fail and (not is_retryable or attempt >= effective_max_retries):
                    update_article_status(article_id, 'ERROR', str(e))
                
                return False
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    final_error = str(last_error) if last_error else 'æœªçŸ¥é”™è¯¯'
    logger.error(f"âŒ çˆ¬å–æœ€ç»ˆå¤±è´¥ {url}: {final_error[:100]}")
    if mark_error_on_fail:
        update_article_status(article_id, 'ERROR', final_error)
    return False


async def crawl_all_articles():
    """çˆ¬å–æ‰€æœ‰æ–‡ç«  - ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘çˆ¬å– + é‡è¯•æœºåˆ¶"""
    global _crawl_progress, _stop_signal
    
    # é‡ç½®åœæ­¢ä¿¡å·ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _stop_signal_lock:
        _stop_signal = False
    
    articles = get_all_articles()
    if not articles:
        logger.info("æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ–‡ç« ")
        reset_crawl_progress()
        return
    
    # è¿‡æ»¤ï¼šåªçˆ¬å–ç™½åå•ä¸­çš„å¹³å°
    filtered_articles = []
    skipped_count = 0
    for article in articles:
        site = article.get('site')
        if is_platform_allowed(site):
            filtered_articles.append(article)
        else:
            skipped_count += 1
            logger.info(f"â­ï¸  è·³è¿‡éç™½åå•å¹³å°: {article.get('url')} (å¹³å°: {site})")
    
    if skipped_count > 0:
        logger.info(f"â­ï¸  å·²è·³è¿‡ {skipped_count} ç¯‡éç™½åå•å¹³å°æ–‡ç« ")
    
    if not filtered_articles:
        logger.info("æ²¡æœ‰éœ€è¦çˆ¬å–çš„æ–‡ç« ï¼ˆæ‰€æœ‰æ–‡ç« éƒ½ä¸åœ¨ç™½åå•ä¸­ï¼‰")
        reset_crawl_progress()
        return
    
    articles = filtered_articles
    
    # æ‰¹é‡è·å–æœ€æ–°é˜…è¯»æ•°ï¼ˆä¼˜åŒ–ï¼šé¿å…åœ¨çˆ¬å–å¾ªç¯ä¸­é€ä¸ªæŸ¥è¯¢ï¼‰
    article_ids = [a['id'] for a in articles]
    latest_counts = get_latest_read_counts_batch(article_ids)
    
    # å°†æœ€æ–°é˜…è¯»æ•°æ·»åŠ åˆ°æ–‡ç« å­—å…¸ä¸­ï¼Œé¿å…åœ¨çˆ¬å–æ—¶é‡å¤æŸ¥è¯¢
    for article in articles:
        latest = latest_counts.get(article['id'])
        article['_latest_count'] = latest['count'] if latest else None

    # æŒ‰ç«™ç‚¹äº¤é”™è°ƒåº¦ï¼ˆround-robinï¼‰ï¼Œä½¿å¹¶å‘æ›´å‡åŒ€åˆ†å¸ƒåœ¨å¤šç«™ç‚¹
    if CRAWL_INTERLEAVE_BY_SITE:
        articles = _interleave_articles_by_site(articles)
        logger.info("å·²æŒ‰ç«™ç‚¹äº¤é”™æ’åºæ–‡ç« ")

    # æ¯åŸŸåçŠ¶æ€åœ¨æœ¬æ¬¡çˆ¬å–å†…ä½¿ç”¨ï¼Œå¼€å§‹æ—¶é‡ç½®
    _reset_domain_semaphores()
    _reset_domain_rate_limit()
    
    # åˆå§‹åŒ–è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    with _crawl_progress_lock:
        _crawl_progress['is_running'] = True
        _crawl_progress['total'] = len(articles)
        _crawl_progress['current'] = 0
        _crawl_progress['success'] = 0
        _crawl_progress['failed'] = 0
        _crawl_progress['retried'] = 0
        _crawl_progress['start_time'] = datetime.now().isoformat()
        _crawl_progress['end_time'] = None
    
    # è®°å½•é˜²åçˆ¬çŠ¶æ€
    if ANTI_SCRAPING_ENABLED:
        logger.info(f"ğŸ›¡ï¸ é˜²åçˆ¬å·²å¯ç”¨: UAè½®æ¢, éšèº«æ¨¡å¼, éšæœºå»¶è¿Ÿ({ANTI_SCRAPING_MIN_DELAY}-{ANTI_SCRAPING_MAX_DELAY}ç§’)")
        # é‡ç½®é˜²åçˆ¬ç®¡ç†å™¨ï¼Œç¡®ä¿æ¯æ¬¡çˆ¬å–ä½¿ç”¨æ–°çš„é…ç½®
        reset_anti_scraping_manager()
    
    logger.info(f"å¼€å§‹çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ï¼ˆå¹¶å‘æ•°: {CRAWL_CONCURRENCY}, æœ€å¤§é‡è¯•: {CRAWL_MAX_RETRIES}ï¼‰")
    start_time = datetime.now()
    
    # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
    semaphore = asyncio.Semaphore(CRAWL_CONCURRENCY)
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨ç‹¬ç«‹æµè§ˆå™¨å®ä¾‹ï¼ˆæ›´ç¨³å®šï¼Œé¿å…å¹¶å‘å†²çªï¼‰
    use_shared_crawler = False
    
    shared_crawler = None
    if use_shared_crawler:
        try:
            shared_crawler = await create_shared_crawler()
            logger.info("ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œæå‡æ€§èƒ½")
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºå…±äº«æµè§ˆå™¨å®ä¾‹ï¼Œä½¿ç”¨ç‹¬ç«‹å®ä¾‹: {e}")
            shared_crawler = None
    
    # ç¬¬ä¸€è½®çˆ¬å–ï¼šåªå°è¯•ä¸€æ¬¡ï¼Œä¸é‡è¯•ï¼ˆå¿«é€Ÿå¤±è´¥ï¼Œæ”¶é›†éœ€è¦é‡è¯•çš„æ–‡ç« ï¼‰
    failed_articles_lock = threading.Lock()  # ä¿æŠ¤å¤±è´¥æ–‡ç« åˆ—è¡¨çš„é”
    failed_articles = []
    
    async def crawl_with_progress(article: dict, index: int):
        """å¸¦è¿›åº¦æ›´æ–°çš„çˆ¬å–ä»»åŠ¡ï¼ˆç¬¬ä¸€è½®ï¼šåªå°è¯•ä¸€æ¬¡ï¼‰ï¼›å«æ¯åŸŸåé™æµä¸ä¿¡å·é‡ã€‚"""
        with _stop_signal_lock:
            if _stop_signal:
                return False

        domain = _domain_from_article(article)
        await _wait_domain_rate_limit(domain)
        domain_sem = await _get_domain_semaphore(domain)

        try:
            if domain_sem is not None:
                async with domain_sem:
                    result = await crawl_article_with_retry(
                        article,
                        crawler=shared_crawler,
                        semaphore=semaphore,
                        max_retries=CRAWL_MAX_RETRIES,
                        skip_retry=True,
                    )
            else:
                result = await crawl_article_with_retry(
                    article,
                    crawler=shared_crawler,
                    semaphore=semaphore,
                    max_retries=CRAWL_MAX_RETRIES,
                    skip_retry=True,
                )

            # æ›´æ–°è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            with _crawl_progress_lock:
                _crawl_progress['current'] = index + 1
                _crawl_progress['current_url'] = article['url']
                if result:
                    _crawl_progress['success'] += 1
                else:
                    _crawl_progress['failed'] += 1

            if not result:
                with failed_articles_lock:
                    failed_articles.append(article)

            if not ANTI_SCRAPING_ENABLED and CRAWL_DELAY > 0 and result:
                await asyncio.sleep(CRAWL_DELAY)

            return result
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {article['url']}: {e}")
            with failed_articles_lock:
                failed_articles.append(article)
            with _crawl_progress_lock:
                _crawl_progress['failed'] += 1
            return False
        finally:
            await _record_domain_request_done(domain)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡ï¼ˆç¬¬ä¸€è½®ï¼‰
    tasks = [crawl_with_progress(article, i) for i, article in enumerate(articles)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†å¼‚å¸¸ç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"ä»»åŠ¡å¼‚å¸¸ {articles[i]['url']}: {result}")
            with failed_articles_lock:
                if articles[i] not in failed_articles:
                    failed_articles.append(articles[i])
            with _crawl_progress_lock:
                _crawl_progress['failed'] += 1
    
    # æ¸…ç†å…±äº«æµè§ˆå™¨å®ä¾‹ï¼ˆç¬¬ä¸€è½®ç»“æŸï¼‰
    if shared_crawler:
        try:
            await shared_crawler.__aexit__(None, None, None)
        except Exception as e:
            logger.warning(f"æ¸…ç†å…±äº«æµè§ˆå™¨å®ä¾‹æ—¶å‡ºé”™: {e}")
            # å°è¯•å¼ºåˆ¶æ¸…ç†
            try:
                if hasattr(shared_crawler, 'browser') and shared_crawler.browser:
                    await shared_crawler.browser.close()
            except Exception as e2:
                logger.debug(f"å¼ºåˆ¶æ¸…ç†æµè§ˆå™¨å®ä¾‹å¤±è´¥: {e2}")
    
    # ç¬¬äºŒè½®ï¼šé›†ä¸­é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ–‡ç« ï¼ˆå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼‰
    # ä¼˜åŒ–ï¼šæŒ‰é”™è¯¯ç±»å‹æ’åºï¼Œä¼˜å…ˆé‡è¯•æˆåŠŸç‡é«˜çš„é”™è¯¯
    def _get_retry_priority(article: dict) -> int:
        """è·å–é‡è¯•ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰"""
        last_error = article.get('last_error', '')
        if not last_error:
            return 2  # æœªçŸ¥é”™è¯¯ï¼Œä¸­ç­‰ä¼˜å…ˆçº§
        
        # åˆ›å»ºä¸´æ—¶å¼‚å¸¸å¯¹è±¡ç”¨äºåˆ†ç±»
        try:
            temp_error = Exception(last_error)
            category = _get_error_category(temp_error)
            return RETRY_PRIORITY_MAP.get(category, 2)
        except Exception:
            return 2
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºå¤±è´¥æ–‡ç« 
    failed_articles.sort(key=_get_retry_priority)
    
    with _stop_signal_lock:
        should_retry = not _stop_signal and len(failed_articles) > 0
    
    if should_retry:
        logger.info(f"ğŸ”„ å¼€å§‹é›†ä¸­é‡è¯• {len(failed_articles)} ç¯‡å¤±è´¥æ–‡ç« ï¼ˆå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼‰")
        
        # ä½¿ç”¨æµè§ˆå™¨æ± è¿›è¡Œé›†ä¸­é‡è¯•
        from .browser_pool import get_browser_pool
        browser_pool = get_browser_pool()
        
        # æ‰¹é‡é‡è¯•ï¼ˆä½¿ç”¨æµè§ˆå™¨æ± ï¼‰
        retry_semaphore = asyncio.Semaphore(CRAWL_CONCURRENCY)
        retry_start_time = datetime.now()
        
        async def retry_with_pool(article: dict, index: int):
            """ä½¿ç”¨æµè§ˆå™¨æ± é‡è¯•å¤±è´¥çš„æ–‡ç« ï¼›å«æ¯åŸŸåé™æµä¸ä¿¡å·é‡ã€‚"""
            with _stop_signal_lock:
                if _stop_signal:
                    return False

            domain = _domain_from_article(article)
            domain_sem = await _get_domain_semaphore(domain)

            try:
                if domain_sem is not None:
                    async with domain_sem:
                        await _wait_domain_rate_limit(domain)
                        return await _do_retry_with_pool(article, index)
                await _wait_domain_rate_limit(domain)
                return await _do_retry_with_pool(article, index)
            finally:
                await _record_domain_request_done(domain)

        async def _do_retry_with_pool(article: dict, index: int) -> bool:
            """å®é™…æ‰§è¡Œé‡è¯•ï¼ˆè·å–æµè§ˆå™¨ã€è°ƒç”¨ _crawl_with_retryã€æ›´æ–°è¿›åº¦ï¼‰ã€‚"""
            try:
                crawler = await browser_pool.acquire()
                use_pool = True
                if not crawler:
                    crawler = await create_shared_crawler()
                    use_pool = False
                try:
                    result = await _crawl_with_retry(article, crawler, CRAWL_MAX_RETRIES, mark_error_on_fail=True)
                    with _crawl_progress_lock:
                        _crawl_progress['current'] = len(articles) + index + 1
                        _crawl_progress['current_url'] = article['url']
                        if result:
                            _crawl_progress['success'] += 1
                            _crawl_progress['failed'] -= 1
                            _crawl_progress['retried'] += 1
                        else:
                            _crawl_progress['retried'] += 1
                    if not result:
                        update_article_status(article['id'], 'ERROR', 'é›†ä¸­é‡è¯•åä»ç„¶å¤±è´¥')
                    return result
                finally:
                    if use_pool:
                        await browser_pool.release(crawler)
                    else:
                        await crawler.__aexit__(None, None, None)
            except Exception as e:
                logger.error(f"é‡è¯•å¼‚å¸¸ {article['url']}: {e}")
                update_article_status(article['id'], 'ERROR', str(e))
                with _crawl_progress_lock:
                    _crawl_progress['retried'] += 1
                return False
        
        # å¹¶å‘é‡è¯•æ‰€æœ‰å¤±è´¥çš„æ–‡ç« 
        retry_tasks = []
        for i, article in enumerate(failed_articles):
            async def retry_with_semaphore(article, index):
                async with retry_semaphore:
                    return await retry_with_pool(article, index)
            retry_tasks.append(retry_with_semaphore(article, i))
        
        retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)
        
        retry_elapsed = (datetime.now() - retry_start_time).total_seconds()
        retry_success = sum(1 for r in retry_results if r is True)
        logger.info(f"ğŸ”„ é›†ä¸­é‡è¯•å®Œæˆ: {retry_success}/{len(failed_articles)} æˆåŠŸ, è€—æ—¶ {retry_elapsed:.2f} ç§’")
    
    # å®Œæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()
    
    with _crawl_progress_lock:
        _crawl_progress['is_running'] = False
        _crawl_progress['end_time'] = end_time.isoformat()
        _crawl_progress['current_url'] = None
        success_count = _crawl_progress['success']
        failed_count = _crawl_progress['failed']
        retried_count = _crawl_progress['retried']
    
    success_rate = (success_count / len(articles) * 100) if articles else 0
    logger.info(f"çˆ¬å–å®Œæˆ: {success_count}/{len(articles)} æˆåŠŸ ({success_rate:.1f}%), "
                f"{failed_count} å¤±è´¥, {retried_count} æ¬¡é‡è¯•, "
                f"è€—æ—¶ {elapsed:.2f} ç§’")
    if elapsed > 0:
        logger.info(f"å¹³å‡é€Ÿåº¦: {len(articles) / elapsed:.2f} æ–‡ç« /ç§’")

def crawl_all_sync():
    """åŒæ­¥åŒ…è£…å™¨ï¼›è‹¥å·²æœ‰çˆ¬å–åœ¨é‹è¡Œå‰‡è·³éï¼ˆé˜²å®šæ™‚ä»»å‹™ç–ŠåŠ ï¼‰"""
    progress = get_crawl_progress()
    if progress.get('is_running'):
        logger.info("çˆ¬å–å·²åœ¨é€²è¡Œä¸­ï¼Œè·³éæœ¬æ¬¡å®šæ™‚è§¸ç™¼")
        return
    try:
        asyncio.run(crawl_all_articles())
    except Exception as e:
        with _crawl_progress_lock:
            _crawl_progress['is_running'] = False
            _crawl_progress['end_time'] = datetime.now().isoformat()
        logger.error(f"çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")