"""
é˜…è¯»æ•°æå–å™¨ - é…ç½®åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å®šä¹‰æå–è§„åˆ™
é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šUser-Agent è½®æ¢ã€éšèº«æ¨¡å¼ã€éšæœºå»¶è¿Ÿ
"""
import re
import logging
from typing import Optional, Dict
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
import asyncio
from .config import (
    PLATFORM_EXTRACTORS,
    ANTI_SCRAPING_ENABLED,
    ANTI_SCRAPING_ROTATE_UA,
    ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_STEALTH_MODE,
    ANTI_SCRAPING_MIN_DELAY,
    ANTI_SCRAPING_MAX_DELAY
)
from .anti_scraping import (
    get_anti_scraping_manager,
    get_random_user_agent,
    get_random_viewport,
    AntiScrapingManager
)

logger = logging.getLogger(__name__)

# é˜²åçˆ¬ç®¡ç†å™¨å®ä¾‹
_anti_scraping_manager: Optional[AntiScrapingManager] = None


def _get_anti_scraping_manager() -> AntiScrapingManager:
    """è·å–é˜²åçˆ¬ç®¡ç†å™¨å•ä¾‹"""
    global _anti_scraping_manager
    if _anti_scraping_manager is None:
        _anti_scraping_manager = get_anti_scraping_manager(
            rotate_user_agent=ANTI_SCRAPING_ROTATE_UA,
            random_delay=ANTI_SCRAPING_RANDOM_DELAY,
            stealth_mode=ANTI_SCRAPING_STEALTH_MODE,
            min_delay=ANTI_SCRAPING_MIN_DELAY,
            max_delay=ANTI_SCRAPING_MAX_DELAY
        )
    return _anti_scraping_manager


def _get_browser_config() -> BrowserConfig:
    """è·å–æµè§ˆå™¨é…ç½®ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰"""
    if ANTI_SCRAPING_ENABLED:
        manager = _get_anti_scraping_manager()
        profile = manager.get_browser_profile()
        
        return BrowserConfig(
            headless=True,
            viewport_width=profile.viewport_width,
            viewport_height=profile.viewport_height,
            user_agent=profile.user_agent,
            verbose=False,
            extra_args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-infobars',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
    else:
        # é»˜è®¤é…ç½®ï¼ˆä¸å¯ç”¨é˜²åçˆ¬ï¼‰
        return BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=800,
            verbose=False
        )


# å…±äº«çš„æµè§ˆå™¨é…ç½®ï¼ˆå¤ç”¨ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å‡½æ•°åŠ¨æ€ç”Ÿæˆï¼Œæ”¯æŒé˜²åçˆ¬
_SHARED_BROWSER_CONFIG = None  # å»¶è¿Ÿåˆå§‹åŒ–


def _ensure_browser_config() -> BrowserConfig:
    """ç¡®ä¿æµè§ˆå™¨é…ç½®å·²åˆå§‹åŒ–"""
    global _SHARED_BROWSER_CONFIG
    if _SHARED_BROWSER_CONFIG is None:
        _SHARED_BROWSER_CONFIG = _get_browser_config()
    return _SHARED_BROWSER_CONFIG


# é»˜è®¤çš„çˆ¬å–é…ç½®
_DEFAULT_CRAWLER_CONFIG = CrawlerRunConfig(
    page_timeout=20000,  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°20ç§’
    remove_overlay_elements=True,
    screenshot=False,  # ç¦ç”¨æˆªå›¾ä»¥æå‡æ€§èƒ½
    wait_for=None,  # ä¸ç­‰å¾…ç‰¹å®šå…ƒç´ ï¼Œç›´æ¥çˆ¬å–
)


async def create_shared_crawler():
    """åˆ›å»ºå…±äº«çš„æµè§ˆå™¨å®ä¾‹ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰"""
    # æ¯æ¬¡åˆ›å»ºæ—¶ç”Ÿæˆæ–°çš„æµè§ˆå™¨é…ç½®ï¼ˆå¦‚æœå¯ç”¨äº† UA è½®æ¢ï¼‰
    if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_ROTATE_UA:
        browser_config = _get_browser_config()
        logger.debug(f"ğŸ›¡ï¸ åˆ›å»ºé˜²åçˆ¬æµè§ˆå™¨å®ä¾‹")
    else:
        browser_config = _ensure_browser_config()
    
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.__aenter__()
    return crawler

def parse_read_count(text: str) -> Optional[int]:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç† k/m/w åç¼€å’Œé€—å·åˆ†éš”ç¬¦
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - çº¯æ•°å­—: "1000" -> 1000
    - å¸¦é€—å·: "1,234" -> 1234
    - kåç¼€: "1k" -> 1000, "20k" -> 20000, "1.5k" -> 1500
    - måç¼€: "1m" -> 1000000, "2.5m" -> 2500000
    - wåç¼€: "1w" -> 10000, "10w" -> 100000
    - æ··åˆ: "1,234.5k" -> 1234500
    
    ç¤ºä¾‹ï¼š
        parse_read_count("1k") -> 1000
        parse_read_count("20k") -> 20000
        parse_read_count("1.5k") -> 1500
    """
    if not text:
        return None
    
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
    text = text.strip().replace(' ', '')
    
    # åŒ¹é…æ•°å­—ï¼ˆæ”¯æŒå°æ•°ç‚¹ã€é€—å·ï¼‰å’Œk/m/wåç¼€
    # æ¨¡å¼è¯´æ˜ï¼š
    #   [\d,]+         åŒ¹é…æ•°å­—å’Œé€—å·ï¼ˆæ•´æ•°éƒ¨åˆ†ï¼‰
    #   (?:\.[\d,]+)?  åŒ¹é…å¯é€‰çš„å°æ•°éƒ¨åˆ†ï¼ˆåŒ…å«å°æ•°ç‚¹ï¼‰
    #   ([kmwKMW]?)    åŒ¹é…å¯é€‰çš„åç¼€ï¼ˆk/m/wï¼Œå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    match = re.search(r'([\d,]+(?:\.[\d,]+)?)([kmwKMW]?)', text)
    if not match:
        return None
    
    number_str = match.group(1)
    suffix = match.group(2).lower()
    
    # ç§»é™¤æ‰€æœ‰é€—å·ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°
    number_str = number_str.replace(',', '')
    
    try:
        number = float(number_str)
    except ValueError:
        return None
    
    # åç¼€å€æ•°æ˜ å°„
    multipliers = {
        'k': 1000,      # åƒ: 1k = 1000, 20k = 20000
        'm': 1000000,   # ç™¾ä¸‡: 1m = 1000000
        'w': 10000      # ä¸‡ï¼ˆä¸­æ–‡ï¼‰: 1w = 10000
    }
    multiplier = multipliers.get(suffix, 1)
    
    # è®¡ç®—æœ€ç»ˆç»“æœå¹¶è½¬æ¢ä¸ºæ•´æ•°
    result = int(number * multiplier)
    return result

async def _crawl_with_shared(url: str, crawler: AsyncWebCrawler, crawler_config: CrawlerRunConfig):
    """ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹çˆ¬å–é¡µé¢ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    
    é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šäººç±»åŒ–å»¶è¿Ÿ
    """
    try:
        # æ‰§è¡Œäººç±»åŒ–å»¶è¿Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_RANDOM_DELAY:
            manager = _get_anti_scraping_manager()
            await manager.human_delay()
        
        result = await crawler.arun(url, config=crawler_config)
        if not result.success:
            return None
        return result
    except Exception as e:
        logger.debug(f"çˆ¬å–å¤±è´¥ {url}: {e}")
        return None

def _parse_number(text: str, method: str = 'number') -> Optional[int]:
    """æ ¹æ®æŒ‡å®šæ–¹æ³•è§£ææ•°å­—
    
    Args:
        text: è¦è§£æçš„æ–‡æœ¬
        method: è§£ææ–¹æ³•
            - 'number': ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒk/m/wåç¼€ï¼‰
            - 'number_with_suffix': æ”¯æŒk/m/wåç¼€ï¼ˆå¦‚ 1k=1000, 20k=20000ï¼‰
    
    Returns:
        è§£æåçš„æ•´æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if not text:
        return None
    
    if method == 'number_with_suffix':
        # ä½¿ç”¨ parse_read_count å¤„ç†å¸¦åç¼€çš„æ•°å­—
        # æ³¨æ„ï¼šparse_read_count å†…éƒ¨ä¼šå¤„ç†ç©ºæ ¼å’Œé€—å·
        return parse_read_count(text)
    else:
        # ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒåç¼€ï¼‰
        # ç§»é™¤ç©ºæ ¼å’Œé€—å·ï¼Œç„¶åæå–ç¬¬ä¸€ä¸ªæ•°å­—
        text = text.strip().replace(' ', '').replace(',', '')
        match = re.search(r'(\d+)', text)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                return None
        return None


def _extract_title_from_html(html: str) -> Optional[str]:
    """ä» HTML ä¸­æå–æ–‡ç« æ ‡é¢˜
    
    ä¼˜å…ˆçº§ï¼š
    1. <title> æ ‡ç­¾
    2. <h1> æ ‡ç­¾
    3. og:title meta æ ‡ç­¾
    """
    if not html:
        return None
    
    # 1. å°è¯•ä» <title> æ ‡ç­¾æå–
    title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
    if title_match:
        title = title_match.group(1).strip()
        # æ¸…ç†å¸¸è§çš„ç½‘ç«™åç¼€
        suffixes_to_remove = [
            r'\s*[-|_â€“â€”]\s*(æ˜é‡‘|CSDN|åšå®¢å›­|51CTO|SegmentFault|ç®€ä¹¦|ç”µå­å‘çƒ§å‹|ä¸éç½‘|FreeBuf).*$',
            r'\s*[-|_â€“â€”]\s*.*åšå®¢.*$',
            r'\s*[-|_â€“â€”]\s*.*æŠ€æœ¯.*$',
        ]
        for suffix in suffixes_to_remove:
            title = re.sub(suffix, '', title, flags=re.IGNORECASE)
        if title:
            return title.strip()
    
    # 2. å°è¯•ä» <h1> æ ‡ç­¾æå–
    h1_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE | re.DOTALL)
    if h1_match:
        title = h1_match.group(1).strip()
        # ç§»é™¤ HTML æ ‡ç­¾
        title = re.sub(r'<[^>]+>', '', title)
        if title:
            return title.strip()
    
    # 3. å°è¯•ä» og:title meta æ ‡ç­¾æå–
    og_match = re.search(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']', html, re.IGNORECASE)
    if og_match:
        return og_match.group(1).strip()
    
    # åå‘åŒ¹é… og:title
    og_match2 = re.search(r'<meta[^>]*content=["\']([^"\']+)["\'][^>]*property=["\']og:title["\']', html, re.IGNORECASE)
    if og_match2:
        return og_match2.group(1).strip()
    
    return None


async def extract_with_config(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        é˜…è¯»æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return None
    
    # è°ƒç”¨å®Œæ•´ç‰ˆæœ¬ï¼Œåªè¿”å›é˜…è¯»æ•°
    read_count, _ = await extract_with_config_full(url, platform, crawler)
    return read_count


async def extract_with_config_full(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> tuple:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°å’Œæ ‡é¢˜
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        (é˜…è¯»æ•°, æ ‡é¢˜) å…ƒç»„ï¼Œå¤±è´¥æ—¶å¯¹åº”å€¼ä¸º None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return (None, None)
    
    config = PLATFORM_EXTRACTORS[platform]
    patterns = config.get('patterns', [])
    wait_for = config.get('wait_for')
    timeout = config.get('timeout', 20000)
    parse_method = config.get('parse_method', 'number')
    delay_before_return = config.get('delay_before_return', 0)  # é¢å¤–å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    js_extract = config.get('js_extract', False)  # æ˜¯å¦ä½¿ç”¨ JavaScript æå–
    
    # è·å–é˜²åçˆ¬ JavaScriptï¼ˆéšèº«æ¨¡å¼ï¼‰
    stealth_js = ""
    if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_STEALTH_MODE:
        manager = _get_anti_scraping_manager()
        stealth_js = manager.get_stealth_js()
    
    # å¯¹äº freebufï¼Œä½¿ç”¨ JavaScript æå–æ•°å­—
    platform_js = ""
    if platform == 'freebuf' and config.get('js_extract', False):
        platform_js = """
        (() => {
            const reviewEl = document.querySelector('.review');
            if (!reviewEl) return null;
            const text = (reviewEl.textContent || reviewEl.innerText || '').trim();
            // æŸ¥æ‰¾è‡³å°‘3ä½çš„æ•°å­—ï¼ˆæ’é™¤ SVG path ä¸­çš„å°æ•°å­—ï¼‰
            const numbers = text.match(/\\b([\\d,]{3,})\\b/g);
            if (numbers && numbers.length > 0) {
                // é€‰æ‹©æœ€å¤§çš„æ•°å­—ï¼ˆæœ€å¯èƒ½æ˜¯é˜…è¯»æ•°ï¼‰
                const maxNum = numbers.reduce((a, b) => {
                    const numA = parseInt(a.replace(/,/g, ''));
                    const numB = parseInt(b.replace(/,/g, ''));
                    return numA > numB ? a : b;
                });
                // å†™å…¥é¡µé¢æ ‡é¢˜ï¼Œæ–¹ä¾¿åç»­æå–
                document.title = 'READ_COUNT:' + maxNum;
                return maxNum;
            }
            return null;
        })();
        """
    
    # åˆå¹¶ JavaScript ä»£ç ï¼šå…ˆæ‰§è¡Œéšèº«è„šæœ¬ï¼Œå†æ‰§è¡Œå¹³å°è„šæœ¬
    combined_js = stealth_js
    if platform_js:
        combined_js = f"{stealth_js}\n{platform_js}" if stealth_js else platform_js
    
    crawler_config = CrawlerRunConfig(
        page_timeout=timeout,
        wait_for=wait_for,
        remove_overlay_elements=True,
        screenshot=False,
        js_code=combined_js if combined_js else None
    )
    
    # ä½¿ç”¨å…±äº«æµè§ˆå™¨æˆ–åˆ›å»ºæ–°å®ä¾‹
    if crawler:
        result = await _crawl_with_shared(url, crawler, crawler_config)
        if result is None:
            return (None, None)
    else:
        async with AsyncWebCrawler(config=_SHARED_BROWSER_CONFIG) as crawler_instance:
            result = await crawler_instance.arun(url, config=crawler_config)
            if not result.success:
                return (None, None)
    
    # å¦‚æœé…ç½®äº†é¢å¤–å»¶è¿Ÿï¼Œç­‰å¾… JavaScript æ¸²æŸ“
    if delay_before_return > 0:
        import asyncio
        await asyncio.sleep(delay_before_return / 1000.0)  # è½¬æ¢ä¸ºç§’
    
    html = result.html
    markdown = result.markdown or ''
    
    # æå‰æå–æ–‡ç« æ ‡é¢˜
    article_title = _extract_title_from_html(html)
    
    # å¯¹äº freebufï¼Œå¦‚æœé…ç½®äº† JavaScript æå–ï¼Œä¼˜å…ˆä»æ ‡é¢˜ä¸­æå–
    if platform == 'freebuf' and js_extract:
        # JavaScript ä»£ç å·²ç»åœ¨çˆ¬å–æ—¶æ‰§è¡Œï¼Œä¼šå°†æ•°å­—å†™å…¥é¡µé¢æ ‡é¢˜
        # è¿™é‡Œæˆ‘ä»¬ä» HTML çš„ <title> æ ‡ç­¾ä¸­æå–
        title_match = re.search(r'<title[^>]*>READ_COUNT:([\d,]+)</title>', html, re.IGNORECASE)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        # å¦‚æœæ ‡é¢˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•´ä¸ª HTML ä¸­æœç´¢
        title_match = re.search(r'READ_COUNT:([\d,]+)', html)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
    
    # å¯¹äº freebufï¼Œå¦‚æœé…ç½®äº† JavaScript æå–ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
    if platform == 'freebuf' and js_code:
        try:
            # å°è¯•ä» result ä¸­è·å– JavaScript æ‰§è¡Œç»“æœ
            # æ³¨æ„ï¼šcrawl4ai å¯èƒ½ä¸ç›´æ¥è¿”å› JS ç»“æœï¼Œéœ€è¦é‡æ–°æ‰§è¡Œ
            # è¿™é‡Œæˆ‘ä»¬å…ˆç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¦‚æœå¤±è´¥å†è€ƒè™‘å…¶ä»–æ–¹æ³•
            pass
        except:
            pass
    
    # å¯¹äº freebufï¼Œå°è¯•ä»é¡µé¢æ ‡é¢˜ä¸­æå–ï¼ˆJavaScript å†™å…¥çš„ï¼‰
    if platform == 'freebuf' and js_extract:
        # å…ˆå°è¯•ä» <title> æ ‡ç­¾ä¸­æå–
        title_match = re.search(r'<title[^>]*>READ_COUNT:([\d,]+)</title>', html, re.IGNORECASE)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        # å¦‚æœæ ‡é¢˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•´ä¸ª HTML ä¸­æœç´¢
        title_match = re.search(r'READ_COUNT:([\d,]+)', html)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
    
    # å¯¹äº freebufï¼Œå°è¯•ä½¿ç”¨ JavaScript ç›´æ¥ä» DOM æå–
    if platform == 'freebuf' and hasattr(result, 'page') and result.page:
        try:
            # ä½¿ç”¨ JavaScript æå– .review å…ƒç´ ä¸­çš„æ•°å­—
            js_code = """
            () => {
                const reviewEl = document.querySelector('.review');
                if (!reviewEl) return null;
                const text = reviewEl.textContent || reviewEl.innerText;
                const match = text.match(/([\\d,]+)/);
                return match ? match[1] : null;
            }
            """
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¿é—® page å¯¹è±¡ï¼Œä½† crawl4ai å¯èƒ½ä¸ç›´æ¥æš´éœ²
            # å…ˆå°è¯•ä» HTML æå–ï¼Œå¦‚æœå¤±è´¥å†è€ƒè™‘å…¶ä»–æ–¹æ³•
        except:
            pass
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæ¨¡å¼
    for pattern in patterns:
        # å…ˆåœ¨ HTML ä¸­æŸ¥æ‰¾ï¼ˆä½¿ç”¨ DOTALL ä»¥åŒ¹é…è·¨è¡Œå†…å®¹ï¼‰
        match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
        if match:
            text = match.group(1).strip()  # å»é™¤é¦–å°¾ç©ºç™½
            count = _parse_number(text, parse_method)
            if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                return (count, article_title)
        
        # å¦‚æœ HTML ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨ markdown ä¸­æŸ¥æ‰¾
        if markdown:
            match = re.search(pattern, markdown, re.IGNORECASE)
            if match:
                text = match.group(1)
                count = _parse_number(text, parse_method)
                if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                    return (count, article_title)
    
    # å¯¹äº freebufï¼Œå¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ JavaScript ä» DOM æå–
    if platform == 'freebuf':
        # å¦‚æœ HTML ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°å­—ï¼Œå°è¯•ä½¿ç”¨ JavaScript ç›´æ¥ä» DOM æå–
        # è¿™éœ€è¦é‡æ–°è®¿é—®é¡µé¢ï¼Œä½†å¯ä»¥è·å–æ¸²æŸ“åçš„å†…å®¹
        if crawler:
            try:
                # ä½¿ç”¨ JavaScript æå–
                js_code = """
                () => {
                    const reviewEl = document.querySelector('.review');
                    if (!reviewEl) return null;
                    const text = reviewEl.textContent || reviewEl.innerText || '';
                    // æŸ¥æ‰¾æ•°å­—ï¼ˆæ’é™¤ SVG path ä¸­çš„æ•°å­—ï¼‰
                    const match = text.match(/\\s([\\d,]{3,})\\s/);
                    return match ? match[1] : null;
                }
                """
                js_config = CrawlerRunConfig(
                    page_timeout=timeout,
                    wait_for=wait_for,
                    remove_overlay_elements=True,
                    screenshot=False,
                    js_code=js_code
                )
                # é‡æ–°çˆ¬å–é¡µé¢ï¼Œä½¿ç”¨ JavaScript æå–æ•°å­—
                js_result = await _crawl_with_shared(url, crawler, js_config)
                if js_result and js_result.success:
                    # ä»æ ‡é¢˜ä¸­æå–æ•°å­—ï¼ˆJavaScript å†™å…¥çš„ï¼‰
                    js_html = js_result.html
                    title_match = re.search(r'<title[^>]*>READ_COUNT:([\d,]+)</title>', js_html, re.IGNORECASE)
                    if title_match:
                        count = _parse_number(title_match.group(1), parse_method)
                        if count is not None and count > 0:
                            return (count, article_title)
                    # å¦‚æœæ ‡é¢˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•´ä¸ª HTML ä¸­æœç´¢
                    title_match = re.search(r'READ_COUNT:([\d,]+)', js_html)
                    if title_match:
                        count = _parse_number(title_match.group(1), parse_method)
                        if count is not None and count > 0:
                            return (count, article_title)
                    # å°è¯•ä» extracted_content è·å–
                    if hasattr(js_result, 'extracted_content') and js_result.extracted_content:
                        try:
                            import json
                            js_data = json.loads(js_result.extracted_content)
                            if js_data:
                                count = _parse_number(js_data, parse_method)
                                if count is not None and count > 0:
                                    return (count, article_title)
                        except:
                            pass
                    # å¦‚æœ extracted_content æ²¡æœ‰ï¼Œä» HTML ä¸­æå–
                    js_html = js_result.html
                    review_section = re.search(r'class="review"[^>]*>.*?</span>', js_html, re.IGNORECASE | re.DOTALL)
                    if review_section:
                        section = review_section.group(0)
                        # å°è¯•åŒ¹é…æ•°å­—
                        num_match = re.search(r'</i>\s*([\d,]+)\s+</span>', section, re.IGNORECASE | re.DOTALL)
                        if num_match:
                            count = _parse_number(num_match.group(1), parse_method)
                            if count is not None and count > 0:
                                return (count, article_title)
            except:
                pass
        
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šåœ¨æ•´ä¸ª HTML ä¸­æœç´¢ .review å…ƒç´ é™„è¿‘çš„æ•°å­—
        review_section = re.search(r'class="review"[^>]*>.*?</span>', html, re.IGNORECASE | re.DOTALL)
        if review_section:
            section = review_section.group(0)
            # ä¼˜å…ˆæŸ¥æ‰¾ </i> å’Œ </span> ä¹‹é—´çš„æ•°å­—ï¼ˆæœ€å¯èƒ½çš„ä½ç½®ï¼‰
            patterns_to_try = [
                r'</i>\s*([\d,]+)\s+</span>',  # æ•°å­—åå¿…é¡»æœ‰ç©ºç™½å­—ç¬¦
                r'</i>\s*([\d,]+)\s*</span>',  # æ•°å­—åå¯ä»¥æœ‰æˆ–æ²¡æœ‰ç©ºç™½å­—ç¬¦
            ]
            
            for pattern in patterns_to_try:
                between_i_and_span = re.search(pattern, section, re.IGNORECASE | re.DOTALL)
                if between_i_and_span:
                    num_str = between_i_and_span.group(1)
                    count = _parse_number(num_str, parse_method)
                    if count is not None and count > 0:
                        return (count, article_title)
    
    return (None, article_title)


async def extract_article_info(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Dict[str, any]:
    """æå–æ–‡ç« ä¿¡æ¯ï¼ˆé˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
    
    Args:
        url: æ–‡ç«  URL
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        
    Returns:
        åŒ…å« 'read_count' å’Œ 'title' çš„å­—å…¸
    """
    domain = urlparse(url).netloc.lower()
    
    # æ ¹æ®åŸŸååŒ¹é…å¹³å°
    platform = None
    for site_domain, site_name in {
        'juejin.cn': 'juejin',
        'csdn.net': 'csdn',
        'cnblogs.com': 'cnblog',
        '51cto.com': '51cto',
        'segmentfault.com': 'segmentfault',
        'jianshu.com': 'jinshu',
        'elecfans.com': 'elecfans',
        'china.com': 'MBB',
        'eefocus.com': 'eefocus',
        'freebuf.com': 'freebuf'
    }.items():
        if site_domain in domain:
            platform = site_name
            break
    
    result = {'read_count': None, 'title': None}
    
    if platform and platform in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, platform, crawler)
    elif 'generic' in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, 'generic', crawler)
    
    return result


async def extract_read_count(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """æ ¹æ®URLè‡ªåŠ¨é€‰æ‹©æå–å™¨ï¼ˆä»…è¿”å›é˜…è¯»æ•°ï¼‰"""
    info = await extract_article_info(url, crawler)
    return info.get('read_count')
