"""
é˜…è¯»æ•°æå–å™¨ - é…ç½®åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å®šä¹‰æå–è§„åˆ™
é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šUser-Agent è½®æ¢ã€éšèº«æ¨¡å¼ã€éšæœºå»¶è¿Ÿ
ä¼˜åŒ–ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œæå‡åŒ¹é…é€Ÿåº¦
"""
import re
import logging
import asyncio
import json
import time
from typing import Optional, Dict, Any
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from functools import lru_cache
from .config import (
    PLATFORM_EXTRACTORS,
    ANTI_SCRAPING_ENABLED,
    ANTI_SCRAPING_ROTATE_UA,
    ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_STEALTH_MODE,
    ANTI_SCRAPING_MIN_DELAY,
    ANTI_SCRAPING_MAX_DELAY
)
from .anti_scraping import (
    get_anti_scraping_manager,
    AntiScrapingManager
)

logger = logging.getLogger(__name__)

# é˜²åçˆ¬ç®¡ç†å™¨å®ä¾‹
_anti_scraping_manager: Optional[AntiScrapingManager] = None


def _get_anti_scraping_manager() -> AntiScrapingManager:
    """è·å–é˜²åçˆ¬ç®¡ç†å™¨å•ä¾‹"""
    global _anti_scraping_manager
    if _anti_scraping_manager is None:
        _anti_scraping_manager = get_anti_scraping_manager(
            rotate_user_agent=ANTI_SCRAPING_ROTATE_UA,
            random_delay=ANTI_SCRAPING_RANDOM_DELAY,
            stealth_mode=ANTI_SCRAPING_STEALTH_MODE,
            min_delay=ANTI_SCRAPING_MIN_DELAY,
            max_delay=ANTI_SCRAPING_MAX_DELAY
        )
    return _anti_scraping_manager


def _get_browser_config() -> BrowserConfig:
    """è·å–æµè§ˆå™¨é…ç½®ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰
    
    ä¼˜åŒ–ï¼š
    - æ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    - ä½¿ç”¨å®Œæ•´çš„ BrowserProfile å’Œ HTTP headers
    - æ•´åˆ AntiScrapingManager çš„é…ç½®
    """
    # åŸºç¡€æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼ˆé€‚ç”¨äºæ‰€æœ‰é…ç½®ï¼‰
    base_extra_args = [
        '--disable-blink-features=AutomationControlled',
        '--disable-infobars',
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-gpu',  # ç¦ç”¨ GPU åŠ é€Ÿï¼ˆheadless æ¨¡å¼ï¼‰
        '--disable-software-rasterizer',  # ç¦ç”¨è½¯ä»¶å…‰æ …åŒ–
        '--disable-extensions',  # ç¦ç”¨æ‰©å±•
        '--disable-plugins',  # ç¦ç”¨æ’ä»¶
        '--disable-images',  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼ˆæå‡é€Ÿåº¦ï¼‰
    ]
    
    if ANTI_SCRAPING_ENABLED:
        manager = _get_anti_scraping_manager()
        profile = manager.get_browser_profile()
        # è·å–å®Œæ•´çš„ HTTP è¯·æ±‚å¤´ï¼ˆåŒ…å« Accept-Languageã€Sec-Ch-Uaã€Referer ç­‰ï¼‰
        headers = manager.get_http_headers()
        
        # æ•´åˆ extra_argsï¼šåˆå¹¶åŸºç¡€å‚æ•°å’Œé˜²åçˆ¬å‚æ•°
        # æ³¨æ„ï¼šwindow-size å·²ç»åœ¨ viewport ä¸­è®¾ç½®ï¼Œä¸éœ€è¦é‡å¤
        extra_args = base_extra_args + [
            '--disable-setuid-sandbox',  # ä» get_browser_config() ä¸­æ·»åŠ 
        ]
        
        return BrowserConfig(
            headless=True,
            viewport_width=profile.viewport_width,
            viewport_height=profile.viewport_height,
            user_agent=profile.user_agent,
            headers=headers,  # æ·»åŠ å®Œæ•´çš„ HTTP è¯·æ±‚å¤´ï¼Œæå‡åæ£€æµ‹èƒ½åŠ›
            verbose=False,
            extra_args=extra_args
        )
    else:
        # é»˜è®¤é…ç½®ï¼ˆä¸å¯ç”¨é˜²åçˆ¬ï¼‰
        return BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=800,
            verbose=False,
            extra_args=base_extra_args
        )

def get_browser_config() -> BrowserConfig:
    """è·å–æµè§ˆå™¨é…ç½®ï¼ˆå…¬å¼€æ¥å£ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼‰"""
    return _get_browser_config()

def ensure_browser_config() -> BrowserConfig:
    """ç¡®ä¿æµè§ˆå™¨é…ç½®å·²åˆå§‹åŒ–ï¼ˆå…¬å¼€æ¥å£ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼‰"""
    return _ensure_browser_config()


# å…±äº«çš„æµè§ˆå™¨é…ç½®ï¼ˆå¤ç”¨ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å‡½æ•°åŠ¨æ€ç”Ÿæˆï¼Œæ”¯æŒé˜²åçˆ¬
# ä¼˜åŒ–ï¼šå¯¹äºé˜²åçˆ¬æ¨¡å¼ï¼Œæ¯æ¬¡è·å–æ–°é…ç½®ä»¥æ”¯æŒè½®æ¢ï¼›å¯¹äºéé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®
_SHARED_BROWSER_CONFIG = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆä»…ç”¨äºéé˜²åçˆ¬æ¨¡å¼ï¼‰


def _ensure_browser_config() -> BrowserConfig:
    """ç¡®ä¿æµè§ˆå™¨é…ç½®å·²åˆå§‹åŒ–
    
    ä¼˜åŒ–ï¼šå¯¹äºé˜²åçˆ¬æ¨¡å¼ï¼Œæ¯æ¬¡è·å–æ–°é…ç½®ä»¥æ”¯æŒè½®æ¢å’ŒæŒ‡çº¹ä¸€è‡´æ€§
    å¯¹äºéé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®ä»¥æå‡æ€§èƒ½
    """
    global _SHARED_BROWSER_CONFIG
    # å¦‚æœå¯ç”¨é˜²åçˆ¬ï¼Œæ¯æ¬¡éƒ½è·å–æ–°é…ç½®ï¼ˆæ”¯æŒè½®æ¢ï¼‰
    if ANTI_SCRAPING_ENABLED:
        return _get_browser_config()
    # éé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®
    if _SHARED_BROWSER_CONFIG is None:
        _SHARED_BROWSER_CONFIG = _get_browser_config()
    return _SHARED_BROWSER_CONFIG


async def create_shared_crawler():
    """åˆ›å»ºå…±äº«çš„æµè§ˆå™¨å®ä¾‹ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰
    
    ä¼˜åŒ–ï¼šä¼˜å…ˆä»æµè§ˆå™¨æ± è·å–ï¼Œå¦‚æœæ± å·²æ»¡åˆ™åˆ›å»ºç‹¬ç«‹å®ä¾‹
    """
    from .browser_pool import get_browser_pool
    browser_pool = get_browser_pool()
    
    # å°è¯•ä»æ± ä¸­è·å–
    crawler = await browser_pool.acquire()
    if crawler:
        return crawler
    
    # æ± å·²æ»¡ï¼Œåˆ›å»ºç‹¬ç«‹å®ä¾‹
    if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_ROTATE_UA:
        browser_config = _get_browser_config()
        logger.debug(f"ğŸ›¡ï¸ åˆ›å»ºé˜²åçˆ¬æµè§ˆå™¨å®ä¾‹ï¼ˆç‹¬ç«‹ï¼‰")
    else:
        browser_config = _ensure_browser_config()
    
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.__aenter__()
    return crawler

def parse_read_count(text: str) -> Optional[int]:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç† k/m/w åç¼€å’Œé€—å·åˆ†éš”ç¬¦
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - çº¯æ•°å­—: "1000" -> 1000
    - å¸¦é€—å·: "1,234" -> 1234
    - kåç¼€: "1k" -> 1000, "20k" -> 20000, "1.5k" -> 1500
    - måç¼€: "1m" -> 1000000, "2.5m" -> 2500000
    - wåç¼€: "1w" -> 10000, "10w" -> 100000
    - æ··åˆ: "1,234.5k" -> 1234500
    
    ç¤ºä¾‹ï¼š
        parse_read_count("1k") -> 1000
        parse_read_count("20k") -> 20000
        parse_read_count("1.5k") -> 1500
    """
    if not text:
        return None
    
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
    text = text.strip().replace(' ', '')
    
    # åŒ¹é…æ•°å­—ï¼ˆæ”¯æŒå°æ•°ç‚¹ã€é€—å·ï¼‰å’Œk/m/wåç¼€
    # æ¨¡å¼è¯´æ˜ï¼š
    #   [\d,]+         åŒ¹é…æ•°å­—å’Œé€—å·ï¼ˆæ•´æ•°éƒ¨åˆ†ï¼‰
    #   (?:\.[\d,]+)?  åŒ¹é…å¯é€‰çš„å°æ•°éƒ¨åˆ†ï¼ˆåŒ…å«å°æ•°ç‚¹ï¼‰
    #   ([kmwKMW]?)    åŒ¹é…å¯é€‰çš„åç¼€ï¼ˆk/m/wï¼Œå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    match = re.search(r'([\d,]+(?:\.[\d,]+)?)([kmwKMW]?)', text)
    if not match:
        return None
    
    number_str = match.group(1)
    suffix = match.group(2).lower()
    
    # ç§»é™¤æ‰€æœ‰é€—å·ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°
    number_str = number_str.replace(',', '')
    
    try:
        number = float(number_str)
    except ValueError:
        return None
    
    # åç¼€å€æ•°æ˜ å°„
    multipliers = {
        'k': 1000,      # åƒ: 1k = 1000, 20k = 20000
        'm': 1000000,   # ç™¾ä¸‡: 1m = 1000000
        'w': 10000      # ä¸‡ï¼ˆä¸­æ–‡ï¼‰: 1w = 10000
    }
    multiplier = multipliers.get(suffix, 1)
    
    # è®¡ç®—æœ€ç»ˆç»“æœå¹¶è½¬æ¢ä¸ºæ•´æ•°
    result = int(number * multiplier)
    return result

async def _crawl_with_shared(url: str, crawler: AsyncWebCrawler, crawler_config: CrawlerRunConfig):
    """ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹çˆ¬å–é¡µé¢ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    
    é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šäººç±»åŒ–å»¶è¿Ÿ
    ä¼˜åŒ–ï¼šåŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„æ—¥å¿—
    """
    try:
        # æ‰§è¡Œäººç±»åŒ–å»¶è¿Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_RANDOM_DELAY:
            manager = _get_anti_scraping_manager()
            await manager.human_delay()
        
        result = await crawler.arun(url, config=crawler_config)
        if not result.success:
            # è®°å½•å¤±è´¥åŸå› ï¼ˆå¦‚æœ result æœ‰é”™è¯¯ä¿¡æ¯ï¼‰
            error_msg = getattr(result, 'error', 'æœªçŸ¥é”™è¯¯')
            logger.debug(f"çˆ¬å–å¤±è´¥ {url}: {error_msg}")
            return None
        return result
    except asyncio.TimeoutError as e:
        logger.warning(f"â±ï¸ çˆ¬å–è¶…æ—¶ {url}: {e}")
        return None
    except ConnectionError as e:
        logger.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯ {url}: {e}")
        return None
    except Exception as e:
        # æ ¹æ®é”™è¯¯ç±»å‹åˆ†ç±»è®°å½•
        error_str = str(e).lower()
        if 'timeout' in error_str or 'timed out' in error_str:
            logger.warning(f"â±ï¸ è¶…æ—¶é”™è¯¯ {url}: {e}")
        elif 'connection' in error_str or 'network' in error_str:
            logger.warning(f"ğŸ”Œ ç½‘ç»œé”™è¯¯ {url}: {e}")
        elif 'ssl' in error_str or 'certificate' in error_str:
            logger.warning(f"ğŸ”’ SSLé”™è¯¯ {url}: {e}")
        else:
            logger.warning(f"âš ï¸ çˆ¬å–å¤±è´¥ {url}: {e}")
        return None

@lru_cache(maxsize=None)  # æ— ç•Œç¼“å­˜ï¼Œå› ä¸ºæ¨¡å¼æ•°é‡æœ‰é™ä¸”å›ºå®š
def _compile_pattern(pattern: str) -> re.Pattern:
    """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç¼“å­˜ç¼–è¯‘ç»“æœï¼Œæå‡æ€§èƒ½ï¼‰"""
    return re.compile(pattern, re.IGNORECASE | re.DOTALL)

def _parse_number(text: str, method: str = 'number') -> Optional[int]:
    """æ ¹æ®æŒ‡å®šæ–¹æ³•è§£ææ•°å­—
    
    Args:
        text: è¦è§£æçš„æ–‡æœ¬
        method: è§£ææ–¹æ³•
            - 'number': ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒk/m/wåç¼€ï¼‰
            - 'number_with_suffix': æ”¯æŒk/m/wåç¼€ï¼ˆå¦‚ 1k=1000, 20k=20000ï¼‰
    
    Returns:
        è§£æåçš„æ•´æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if not text:
        return None
    
    if method == 'number_with_suffix':
        # ä½¿ç”¨ parse_read_count å¤„ç†å¸¦åç¼€çš„æ•°å­—
        # æ³¨æ„ï¼šparse_read_count å†…éƒ¨ä¼šå¤„ç†ç©ºæ ¼å’Œé€—å·
        return parse_read_count(text)
    else:
        # ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒåç¼€ï¼‰
        # ç§»é™¤ç©ºæ ¼å’Œé€—å·ï¼Œç„¶åæå–ç¬¬ä¸€ä¸ªæ•°å­—
        text = text.strip().replace(' ', '').replace(',', '')
        match = re.search(r'(\d+)', text)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                return None
        return None


# é¢„ç¼–è¯‘æ ‡é¢˜æå–çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
_TITLE_PATTERNS = {
    'title': re.compile(r'<title[^>]*>([^<]+)</title>', re.IGNORECASE),
    'h1': re.compile(r'<h1[^>]*>([^<]+)</h1>', re.IGNORECASE | re.DOTALL),
    'og_title1': re.compile(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']', re.IGNORECASE),
    'og_title2': re.compile(r'<meta[^>]*content=["\']([^"\']+)["\'][^>]*property=["\']og:title["\']', re.IGNORECASE),
}
_TITLE_SUFFIX_PATTERNS = [
    re.compile(r'\s*[-|_â€“â€”]\s*(æ˜é‡‘|CSDN|åšå®¢å›­|51CTO|SegmentFault|ç®€ä¹¦|ç”µå­å‘çƒ§å‹|ä¸éç½‘).*$', re.IGNORECASE),
    re.compile(r'\s*[-|_â€“â€”]\s*.*åšå®¢.*$', re.IGNORECASE),
    re.compile(r'\s*[-|_â€“â€”]\s*.*æŠ€æœ¯.*$', re.IGNORECASE),
]
_HTML_TAG_PATTERN = re.compile(r'<[^>]+>')

def _extract_title_from_html(html: str) -> Optional[str]:
    """ä» HTML ä¸­æå–æ–‡ç« æ ‡é¢˜ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. <title> æ ‡ç­¾
    2. <h1> æ ‡ç­¾
    3. og:title meta æ ‡ç­¾
    """
    if not html:
        return None
    
    # 1. å°è¯•ä» <title> æ ‡ç­¾æå–
    title_match = _TITLE_PATTERNS['title'].search(html)
    if title_match:
        title = title_match.group(1).strip()
        # æ¸…ç†å¸¸è§çš„ç½‘ç«™åç¼€ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        for suffix_pattern in _TITLE_SUFFIX_PATTERNS:
            title = suffix_pattern.sub('', title)
        if title:
            return title.strip()
    
    # 2. å°è¯•ä» <h1> æ ‡ç­¾æå–
    h1_match = _TITLE_PATTERNS['h1'].search(html)
    if h1_match:
        title = h1_match.group(1).strip()
        # ç§»é™¤ HTML æ ‡ç­¾
        title = _HTML_TAG_PATTERN.sub('', title)
        if title:
            return title.strip()
    
    # 3. å°è¯•ä» og:title meta æ ‡ç­¾æå–
    og_match = _TITLE_PATTERNS['og_title1'].search(html)
    if og_match:
        return og_match.group(1).strip()
    
    # åå‘åŒ¹é… og:title
    og_match2 = _TITLE_PATTERNS['og_title2'].search(html)
    if og_match2:
        return og_match2.group(1).strip()
    
    return None


async def extract_with_config(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        é˜…è¯»æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return None
    
    # è°ƒç”¨å®Œæ•´ç‰ˆæœ¬ï¼Œåªè¿”å›é˜…è¯»æ•°
    read_count, _ = await extract_with_config_full(url, platform, crawler)
    return read_count


async def extract_with_config_full(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> tuple:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°å’Œæ ‡é¢˜
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        (é˜…è¯»æ•°, æ ‡é¢˜) å…ƒç»„ï¼Œå¤±è´¥æ—¶å¯¹åº”å€¼ä¸º None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return (None, None)
    
    config = PLATFORM_EXTRACTORS[platform]
    patterns = config.get('patterns', [])
    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæå‡æ€§èƒ½ï¼‰
    # HTML ä½¿ç”¨ DOTALL æ¨¡å¼ï¼ˆæ”¯æŒè·¨è¡ŒåŒ¹é…ï¼‰ï¼Œmarkdown ä¸ä½¿ç”¨
    compiled_patterns_html = [_compile_pattern(p) for p in patterns]
    compiled_patterns_markdown = [re.compile(p, re.IGNORECASE) for p in patterns]
    wait_for = config.get('wait_for')
    timeout = config.get('timeout', 20000)
    parse_method = config.get('parse_method', 'number')
    delay_before_return = config.get('delay_before_return', 0)  # é¢å¤–å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    js_extract = config.get('js_extract', False)  # æ˜¯å¦ä½¿ç”¨ JavaScript æå–
    
    # è·å–é˜²åçˆ¬é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    base_crawler_config = {}
    js_parts = []
    
    if ANTI_SCRAPING_ENABLED:
        manager = _get_anti_scraping_manager()
        # è·å–åŸºç¡€é˜²åçˆ¬é…ç½®
        base_crawler_config = manager.get_crawler_config(
            timeout=timeout,
            wait_for=wait_for
        )
        # å¦‚æœé˜²åçˆ¬é…ç½®ä¸­æœ‰ js_codeï¼Œæ·»åŠ åˆ° js_parts
        if base_crawler_config.get('js_code'):
            js_parts.append(base_crawler_config['js_code'])
            # ç§»é™¤ js_codeï¼Œç¨ååˆå¹¶æ‰€æœ‰ JS ä»£ç 
            base_crawler_config.pop('js_code', None)
    
    # å¹³å°ç‰¹å®šçš„ JavaScript æå–é€»è¾‘
    if js_extract and platform == 'sohu':
        # æœç‹ï¼šwait_for å·²ç¡®ä¿æ•°å­—åŠ è½½å®Œæˆï¼Œè¿™é‡Œç›´æ¥æå–å¹¶æ³¨å…¥æ ‡è®°
        platform_js = """
        (() => {
            const pvEl = document.querySelector('em[data-role="pv"]');
            if (pvEl) {
                const text = pvEl.textContent.trim();
                if (/^\\d+$/.test(text)) {
                    // åœ¨ HTML ä¸­æ³¨å…¥æ˜ç¡®çš„æ ‡è®°ï¼Œç¡®ä¿èƒ½è¢«æ­£åˆ™æå–
                    const marker = document.createElement('script');
                    marker.type = 'text/plain';
                    marker.id = 'sohu-pv-marker';
                    marker.textContent = 'SOHU_PV_COUNT:' + text;
                    document.head.appendChild(marker);
                    return text;
                }
            }
            return null;
        })();
        """
        js_parts.append(platform_js)
    elif js_extract and platform == 'juejin':
        # æ˜é‡‘ï¼šæ‹¦æˆªç½‘ç»œè¯·æ±‚å¹¶ç›´æ¥ä¿®æ”¹ DOM
        platform_js = """
        (() => {
            // æ³¨å…¥æ‰§è¡Œæ ‡è®°
            try {
                const execMarker = document.createElement('div');
                execMarker.id = 'juejin-js-executed';
                execMarker.style.display = 'none';
                execMarker.textContent = 'JUEJIN_JS_EXECUTED';
                if (document.body) {
                    document.body.appendChild(execMarker);
                } else if (document.head) {
                    document.head.appendChild(execMarker);
                }
            } catch (e) {}
            
            const updateViewsCount = (value) => {
                const cleanText = String(value).replace(/,/g, '');
                // ç›´æ¥ä¿®æ”¹ views-count å…ƒç´ çš„å†…å®¹
                const viewsEl = document.querySelector('.views-count');
                if (viewsEl) {
                    viewsEl.textContent = cleanText;
                    // åŒæ—¶æ³¨å…¥æ ‡è®°
                    const marker = document.createElement('script');
                    marker.type = 'text/plain';
                    marker.id = 'juejin-views-marker';
                    marker.textContent = 'JUEJIN_VIEWS_COUNT:' + cleanText;
                    if (document.head) {
                        document.head.appendChild(marker);
                    }
                    return cleanText;
                }
                return null;
            };
            
            // æ‹¦æˆª fetch è¯·æ±‚ï¼ŒæŸ¥æ‰¾åŒ…å«é˜…è¯»æ•°çš„å“åº”
            const originalFetch = window.fetch;
            window.fetch = function(...args) {
                return originalFetch.apply(this, args).then(response => {
                    // å…‹éš†å“åº”ä»¥ä¾¿è¯»å–
                    const clonedResponse = response.clone();
                    clonedResponse.json().then(data => {
                        // æŸ¥æ‰¾åŒ…å« viewCount, views, readCount ç­‰å­—æ®µçš„æ•°æ®
                        const findViewCount = (obj) => {
                            if (typeof obj === 'object' && obj !== null) {
                                for (const key in obj) {
                                    if (key.toLowerCase().includes('view') || key.toLowerCase().includes('read')) {
                                        const value = obj[key];
                                        if (typeof value === 'number' && value > 0 && value < 1000) {
                                            updateViewsCount(value);
                                            return;
                                        }
                                    }
                                    findViewCount(obj[key]);
                                }
                            }
                        };
                        findViewCount(data);
                    }).catch(() => {});
                    return response;
                });
            };
            
            // æ‹¦æˆª XMLHttpRequest
            const originalOpen = XMLHttpRequest.prototype.open;
            const originalSend = XMLHttpRequest.prototype.send;
            XMLHttpRequest.prototype.open = function(method, url, ...args) {
                this._url = url;
                return originalOpen.apply(this, [method, url, ...args]);
            };
            XMLHttpRequest.prototype.send = function(...args) {
                this.addEventListener('load', function() {
                    if (this.responseText) {
                        try {
                            const data = JSON.parse(this.responseText);
                            const findViewCount = (obj) => {
                                if (typeof obj === 'object' && obj !== null) {
                                    for (const key in obj) {
                                        if (key.toLowerCase().includes('view') || key.toLowerCase().includes('read')) {
                                            const value = obj[key];
                                            if (typeof value === 'number' && value > 0 && value < 1000) {
                                                updateViewsCount(value);
                                                return;
                                            }
                                        }
                                        findViewCount(obj[key]);
                                    }
                                }
                            };
                            findViewCount(data);
                        } catch (e) {}
                    }
                });
                return originalSend.apply(this, args);
            };
            
            const findViewsCount = () => {
                const viewsEl = document.querySelector('.views-count');
                if (viewsEl) {
                    const text = viewsEl.textContent.trim();
                    // å¦‚æœå·²ç»æœ‰éé›¶æ•°å­—ï¼Œç›´æ¥ä½¿ç”¨
                    if (text && text !== '0' && /^[\\d,]+$/.test(text)) {
                        return updateViewsCount(text);
                    }
                    // å¦‚æœå†…å®¹æ˜¯ "0"ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹æŸ¥æ‰¾
                    if (text === '0') {
                        // æ£€æŸ¥ data å±æ€§
                        const dataAttrs = viewsEl.getAttributeNames().filter(name => name.startsWith('data-'));
                        for (const attr of dataAttrs) {
                            const value = viewsEl.getAttribute(attr);
                            if (value && /^\\d+$/.test(value) && parseInt(value) > 0 && parseInt(value) < 1000) {
                                return updateViewsCount(value);
                            }
                        }
                        // æŸ¥æ‰¾çˆ¶å…ƒç´ ä¸­çš„æ•°å­—
                        const parent = viewsEl.parentElement;
                        if (parent) {
                            const parentText = parent.textContent.trim();
                            const numbers = parentText.match(/\\b(\\d+)\\b/g);
                            if (numbers) {
                                for (const num of numbers) {
                                    const numVal = parseInt(num);
                                    if (numVal > 0 && numVal < 1000) {
                                        return updateViewsCount(numVal);
                                    }
                                }
                            }
                        }
                    }
                }
                return null;
            };
            
            // ç«‹å³å°è¯•æŸ¥æ‰¾
            findViewsCount();
            
            // ä½¿ç”¨ MutationObserver ç›‘å¬å˜åŒ–
            const viewsEl = document.querySelector('.views-count');
            if (viewsEl) {
                const observer = new MutationObserver(() => {
                    findViewsCount();
                });
                observer.observe(viewsEl, { childList: true, characterData: true, subtree: true, attributes: true });
                
                // è®¾ç½®è¶…æ—¶
                setTimeout(() => {
                    observer.disconnect();
                    findViewsCount(); // æœ€åå°è¯•ä¸€æ¬¡
                }, 5000);
            }
            
            // å»¶è¿Ÿæ£€æŸ¥ï¼ˆç»™é¡µé¢æ—¶é—´åŠ è½½ï¼‰
            setTimeout(() => findViewsCount(), 2000);
            setTimeout(() => findViewsCount(), 4000);
            
            return null;
        })();
        """
        js_parts.append(platform_js)
    
    # åˆå¹¶ JavaScript ä»£ç ï¼šå…ˆæ‰§è¡Œéšèº«è„šæœ¬ï¼Œå†æ‰§è¡Œå¹³å°è„šæœ¬
    combined_js = '\n'.join(js_parts) if js_parts else None
    # åˆ›å»ºçˆ¬å–é…ç½®ï¼ˆæ•´åˆé˜²åçˆ¬é…ç½®å’Œå¹³å°ç‰¹å®šé…ç½®ï¼‰
    crawler_config = CrawlerRunConfig(
        page_timeout=timeout,
        wait_for=wait_for,
        remove_overlay_elements=base_crawler_config.get('remove_overlay_elements', True),  # ç§»é™¤å¼¹çª—å’Œé®ç½©å±‚
        screenshot=base_crawler_config.get('screenshot', False),  # ç¦ç”¨æˆªå›¾ä»¥æå‡æ€§èƒ½
        js_code=combined_js if combined_js else None
    )
    
    # ä½¿ç”¨å…±äº«æµè§ˆå™¨æˆ–åˆ›å»ºæ–°å®ä¾‹
    if crawler:
        # ä½¿ç”¨ä¼ å…¥çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        result = await _crawl_with_shared(url, crawler, crawler_config)
        if result is None:
            return (None, None)
    else:
        # æ²¡æœ‰ä¼ å…¥ crawlerï¼Œå°è¯•ä»æµè§ˆå™¨æ± è·å–æˆ–åˆ›å»ºä¸´æ—¶å®ä¾‹
        from .browser_pool import get_browser_pool
        browser_pool = get_browser_pool()
        
        # å°è¯•ä»æ± ä¸­è·å–
        pool_crawler = await browser_pool.acquire()
        if pool_crawler:
            try:
                result = await _crawl_with_shared(url, pool_crawler, crawler_config)
                if result is None:
                    return (None, None)
            finally:
                # ç¡®ä¿é‡Šæ”¾å›æ± ä¸­
                await browser_pool.release(pool_crawler)
        else:
            # æ± å·²æ»¡ï¼Œåˆ›å»ºä¸´æ—¶å®ä¾‹ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿æ­£ç¡®æ¸…ç†ï¼‰
            browser_config = _ensure_browser_config()
            async with AsyncWebCrawler(config=browser_config) as temp_crawler:
                # ä½¿ç”¨ _crawl_with_shared ç¡®ä¿å¼‚å¸¸å¤„ç†ä¸€è‡´æ€§
                result = await _crawl_with_shared(url, temp_crawler, crawler_config)
                if result is None:
                    return (None, None)
    
    # å¦‚æœé…ç½®äº†é¢å¤–å»¶è¿Ÿï¼Œç­‰å¾… JavaScript æ¸²æŸ“
    if delay_before_return > 0:
        await asyncio.sleep(delay_before_return / 1000.0)  # è½¬æ¢ä¸ºç§’
    
    html = result.html
    markdown = result.markdown or ''
    # æ£€æµ‹éªŒè¯ç ï¼ˆéƒ¨åˆ†ç½‘ç«™çš„åçˆ¬æœºåˆ¶ï¼‰
    captcha_indicators = ['è®¿é—®éªŒè¯', 'è¯·æŒ‰ä½æ»‘å—', 'æ‹–åŠ¨åˆ°æœ€å³è¾¹', 'æ»‘å—éªŒè¯', 'CAPTCHA_DETECTED']
    for indicator in captcha_indicators:
        if indicator in html:
            logger.warning(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæ— æ³•æå–: {url}")
            return (None, None)
    
    # æå‰æå–æ–‡ç« æ ‡é¢˜
    article_title = _extract_title_from_html(html)
    
    # å¦‚æœé…ç½®äº† JavaScript æå–ï¼Œä¼˜å…ˆä»æ ‡è®°ä¸­æå–ï¼ˆæ”¯æŒ sohuã€juejin ç­‰ï¼‰
    if js_extract:
        # æ–¹æ³•1: ä» JUEJIN_VIEWS_COUNT æ ‡è®°æå–ï¼ˆæ˜é‡‘ä¸“ç”¨ï¼‰
        juejin_match = re.search(r'JUEJIN_VIEWS_COUNT:([\d,]+)', html)
        if juejin_match:
            count = _parse_number(juejin_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        
        # å¦‚æœ JavaScript æ ‡è®°æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» HTML ä¸­ç›´æ¥æŸ¥æ‰¾
        # æŸ¥æ‰¾ .views-count å…ƒç´ é™„è¿‘çš„æ•°å­—ï¼ˆå¯èƒ½åœ¨ data å±æ€§æˆ–å…¶ä»–ä½ç½®ï¼‰
        if platform == 'juejin':
            # é¦–å…ˆå°è¯•ä» JSON æ•°æ®ä¸­æŸ¥æ‰¾ï¼ˆæ˜é‡‘å¯èƒ½å°†é˜…è¯»æ•°å­˜å‚¨åœ¨ JSON ä¸­ï¼‰
            json_matches = re.finditer(r'<script[^>]*type=["\']application/json["\'][^>]*>(.*?)</script>', html, re.DOTALL)
            for json_match in json_matches:
                try:
                    import json as json_lib
                    json_data = json_lib.loads(json_match.group(1))
                    # é€’å½’æŸ¥æ‰¾ JSON ä¸­çš„æ•°å­—å­—æ®µï¼ˆå¯èƒ½æ˜¯ viewCount, views, readCount ç­‰ï¼‰
                    def find_view_count(obj, path=""):
                        if isinstance(obj, dict):
                            for key, value in obj.items():
                                if key.lower() in ['viewcount', 'views', 'readcount', 'read_count', 'view_count'] and isinstance(value, (int, str)):
                                    try:
                                        num_val = int(value) if isinstance(value, str) else value
                                        if num_val > 0 and num_val < 1000000:
                                            return num_val
                                    except:
                                        pass
                                result = find_view_count(value, f"{path}.{key}")
                                if result:
                                    return result
                        elif isinstance(obj, list):
                            for i, item in enumerate(obj):
                                result = find_view_count(item, f"{path}[{i}]")
                                if result:
                                    return result
                        return None
                    
                        view_count = find_view_count(json_data)
                    if view_count:
                        return (view_count, article_title)
                except:
                    pass
            
            # å°è¯•ä» script æ ‡ç­¾ä¸­çš„ JavaScript å˜é‡ä¸­æŸ¥æ‰¾
            # ä¼˜å…ˆæŸ¥æ‰¾è¾ƒå°çš„æ•°å­—ï¼ˆå› ä¸ºå®é™…é˜…è¯»æ•°å¯èƒ½æ˜¯ 9 è¿™æ ·çš„å°æ•°å­—ï¼‰
            script_matches = re.finditer(r'<script[^>]*>(.*?)</script>', html, re.DOTALL)
            candidates = []  # å­˜å‚¨æ‰€æœ‰å€™é€‰æ•°å­—
            for script_match in script_matches:
                script_content = script_match.group(1)
                # æŸ¥æ‰¾å¯èƒ½çš„é˜…è¯»æ•°å˜é‡ï¼ˆå¦‚ viewCount, views, readCount ç­‰ï¼‰
                view_count_patterns = [
                    r'viewCount["\']?\s*[:=]\s*(\d+)',
                    r'views["\']?\s*[:=]\s*(\d+)',
                    r'readCount["\']?\s*[:=]\s*(\d+)',
                    r'view_count["\']?\s*[:=]\s*(\d+)',
                    r'read_count["\']?\s*[:=]\s*(\d+)',
                ]
                for pattern in view_count_patterns:
                    match = re.search(pattern, script_content, re.IGNORECASE)
                    if match:
                        num_val = int(match.group(1))
                        if num_val > 0 and num_val < 1000000:
                            candidates.append((num_val, pattern))
            
            # ä¼˜å…ˆè¿”å›è¾ƒå°çš„æ•°å­—ï¼ˆå°äº 1000ï¼‰ï¼Œå¦‚æœæ‰¾ä¸åˆ°ï¼Œå†è¿”å›è¾ƒå¤§çš„æ•°å­—
            if candidates:
                # å…ˆå°è¯•è¾ƒå°çš„æ•°å­—
                small_candidates = [c for c in candidates if c[0] < 1000]
                if small_candidates:
                    # è¿”å›æœ€å°çš„æ•°å­—ï¼ˆæœ€å¯èƒ½æ˜¯æ­£ç¡®çš„é˜…è¯»æ•°ï¼‰
                    num_val, pattern = min(small_candidates, key=lambda x: x[0])
                    return (num_val, article_title)
                else:
                    # å¦‚æœæ²¡æœ‰å°æ•°å­—ï¼Œè¿”å›æœ€å°çš„æ•°å­—
                    num_val, pattern = min(candidates, key=lambda x: x[0])
                    return (num_val, article_title)
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« views-count çš„å…ƒç´ 
            views_count_pattern = r'<[^>]*class="[^"]*views-count[^"]*"[^>]*>'
            views_count_matches = list(re.finditer(views_count_pattern, html))
            for match in views_count_matches:
                # è·å–å…ƒç´ åŠå…¶å±æ€§
                element_start = match.start()
                element_end = html.find('>', element_start) + 1
                element_html = html[element_start:element_end]
                # æŸ¥æ‰¾ data å±æ€§ä¸­çš„æ•°å­—
                data_attr_match = re.search(r'data-[^=]*="(\d+)"', element_html)
                if data_attr_match:
                    count = _parse_number(data_attr_match.group(1), parse_method)
                    if count is not None and count > 0 and count < 1000000:
                        return (count, article_title)
                
                # å¦‚æœ data å±æ€§ä¸­æ²¡æœ‰ï¼ŒæŸ¥æ‰¾å…ƒç´ æ ‡ç­¾ç»“æŸåçš„å†…å®¹ï¼ˆå…ƒç´ å†…éƒ¨ï¼‰
                # æŸ¥æ‰¾ </span> æ ‡ç­¾ä¹‹å‰çš„å†…å®¹ï¼Œè¿™åº”è¯¥æ˜¯å…ƒç´ å†…éƒ¨çš„æ–‡æœ¬
                element_close_tag = html.find('</span>', element_end)
                if element_close_tag != -1:
                    # è·å–å…ƒç´ å†…éƒ¨çš„å†…å®¹
                    element_content = html[element_end:element_close_tag]
                    # æŸ¥æ‰¾å…ƒç´ å†…éƒ¨çš„æ•°å­—ï¼ˆæ’é™¤ç©ºç™½å­—ç¬¦ï¼‰
                    # åŒ¹é…æ¨¡å¼ï¼š> åé¢è·Ÿç€ç©ºç™½å­—ç¬¦ï¼Œç„¶åæ˜¯æ•°å­—ï¼Œç„¶åæ˜¯ç©ºç™½å­—ç¬¦ï¼Œç„¶åæ˜¯ </span>
                    inner_match = re.search(r'>\s*(\d+)\s*</span>', html[element_start:element_close_tag + 7])
                    if inner_match:
                        num_str = inner_match.group(1)
                        num_val = int(num_str)
                        # å¦‚æœæ•°å­—ä¸æ˜¯ 0ï¼Œä¸”æ˜¯åˆç†çš„é˜…è¯»æ•°ï¼ˆ1-999999ï¼‰
                        if num_val > 0 and num_val < 1000000:
                            return (num_val, article_title)
                    
                    # å¦‚æœå…ƒç´ å†…å®¹ä»ç„¶æ˜¯ "0"ï¼ŒæŸ¥æ‰¾ç´§é‚»çš„å…„å¼Ÿå…ƒç´ æˆ–çˆ¶å…ƒç´ 
                    # æŸ¥æ‰¾çˆ¶å…ƒç´ ï¼ˆæŸ¥æ‰¾åŒ…å« views-count çš„çˆ¶å®¹å™¨ï¼‰
                    parent_start = html.rfind('<', 0, element_start)
                    if parent_start != -1:
                        # æŸ¥æ‰¾çˆ¶å…ƒç´ çš„ç»“æŸæ ‡ç­¾
                        parent_end = html.find('>', parent_start) + 1
                        # æŸ¥æ‰¾çˆ¶å…ƒç´ ç»“æŸæ ‡ç­¾åçš„å†…å®¹ï¼ˆæœ€å¤š 100 å­—ç¬¦ï¼‰
                        parent_content = html[parent_end:min(len(html), parent_end + 100)]
                        # æŸ¥æ‰¾ç´§é‚»çš„æ•°å­—ï¼ˆåœ¨çˆ¶å…ƒç´ ç»“æŸæ ‡ç­¾åï¼Œä¸‹ä¸€ä¸ªæ ‡ç­¾å‰ï¼‰
                        next_tag_pos = parent_content.find('<')
                        if next_tag_pos != -1:
                            parent_text = parent_content[:next_tag_pos]
                            # æŸ¥æ‰¾æ–‡æœ¬ä¸­çš„æ•°å­—
                            text_numbers = re.findall(r'\b(\d+)\b', parent_text)
                            for num_str in text_numbers:
                                num_val = int(num_str)
                                # å¦‚æœæ•°å­—æ˜¯åˆç†çš„é˜…è¯»æ•°ï¼ˆ1-999999ï¼‰ï¼Œä¸”ä¸æ˜¯ 0
                                if num_val > 0 and num_val < 1000000:
                                    return (num_val, article_title)
        
        # æ–¹æ³•2: ä» READ_COUNT æ ‡è®°æå–
        title_match = re.search(r'READ_COUNT:([\d,]+)', html)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        
        # æ–¹æ³•3: ä» SOHU_READ_COUNT æ ‡è®°æå–ï¼ˆæœç‹ä¸“ç”¨ï¼Œæ”¯æŒ HTML æ³¨é‡Šæ ¼å¼ï¼‰
        sohu_match = re.search(r'SOHU_READ_COUNT:([\d,]+)', html)
        if sohu_match:
            count = _parse_number(sohu_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        
        # æ–¹æ³•4: ä» SOHU_PV_COUNT æ ‡è®°æå–ï¼ˆæœç‹ä¸“ç”¨ï¼‰
        sohu_pv_match = re.search(r'SOHU_PV_COUNT:(\d+)', html)
        if sohu_pv_match:
            count = _parse_number(sohu_pv_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæ¨¡å¼ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    for i, compiled_pattern_html in enumerate(compiled_patterns_html):
        # å…ˆåœ¨ HTML ä¸­æŸ¥æ‰¾
        match = compiled_pattern_html.search(html)
        if match:
            text = match.group(1).strip()  # å»é™¤é¦–å°¾ç©ºç™½
            count = _parse_number(text, parse_method)
            if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                return (count, article_title)
        
        # å¦‚æœ HTML ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨ markdown ä¸­æŸ¥æ‰¾ï¼ˆä½¿ç”¨å¯¹åº”çš„é¢„ç¼–è¯‘æ¨¡å¼ï¼‰
        if markdown:
            compiled_pattern_md = compiled_patterns_markdown[i]
            match = compiled_pattern_md.search(markdown)
            if match:
                text = match.group(1)
                count = _parse_number(text, parse_method)
                if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                    return (count, article_title)
    
    # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥ï¼Œè¿”å› None
    return (None, article_title)


async def extract_article_info(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Dict[str, Any]:
    """æå–æ–‡ç« ä¿¡æ¯ï¼ˆé˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
    
    Args:
        url: æ–‡ç«  URL
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        
    Returns:
        åŒ…å« 'read_count' å’Œ 'title' çš„å­—å…¸
    """
    from .config import SUPPORTED_SITES
    
    domain = urlparse(url).netloc.lower()
    
    # æ ¹æ®åŸŸååŒ¹é…å¹³å°ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ˜ å°„ï¼‰
    platform = None
    for site_domain, site_name in SUPPORTED_SITES.items():
        if site_domain in domain:
            platform = site_name
            break
    
    result = {'read_count': None, 'title': None}
    
    if platform and platform in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, platform, crawler)
    elif 'generic' in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, 'generic', crawler)
    
    return result


async def extract_read_count(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """æ ¹æ®URLè‡ªåŠ¨é€‰æ‹©æå–å™¨ï¼ˆä»…è¿”å›é˜…è¯»æ•°ï¼‰"""
    info = await extract_article_info(url, crawler)
    return info.get('read_count')
