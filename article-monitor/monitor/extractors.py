"""
é˜…è¯»æ•°æå–å™¨ - é…ç½®åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å®šä¹‰æå–è§„åˆ™
é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šUser-Agent è½®æ¢ã€éšèº«æ¨¡å¼ã€éšæœºå»¶è¿Ÿ
ä¼˜åŒ–ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œæå‡åŒ¹é…é€Ÿåº¦
"""
import re
import logging
import asyncio
from typing import Optional, Dict, Any
from urllib.parse import urlparse
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from functools import lru_cache
from .config import (
    PLATFORM_EXTRACTORS,
    ANTI_SCRAPING_ENABLED,
    ANTI_SCRAPING_ROTATE_UA,
    ANTI_SCRAPING_RANDOM_DELAY,
    ANTI_SCRAPING_STEALTH_MODE,
    ANTI_SCRAPING_MIN_DELAY,
    ANTI_SCRAPING_MAX_DELAY
)
from .anti_scraping import (
    get_anti_scraping_manager,
    AntiScrapingManager
)

logger = logging.getLogger(__name__)

# é˜²åçˆ¬ç®¡ç†å™¨å®ä¾‹
_anti_scraping_manager: Optional[AntiScrapingManager] = None


def _get_anti_scraping_manager() -> AntiScrapingManager:
    """è·å–é˜²åçˆ¬ç®¡ç†å™¨å•ä¾‹"""
    global _anti_scraping_manager
    if _anti_scraping_manager is None:
        _anti_scraping_manager = get_anti_scraping_manager(
            rotate_user_agent=ANTI_SCRAPING_ROTATE_UA,
            random_delay=ANTI_SCRAPING_RANDOM_DELAY,
            stealth_mode=ANTI_SCRAPING_STEALTH_MODE,
            min_delay=ANTI_SCRAPING_MIN_DELAY,
            max_delay=ANTI_SCRAPING_MAX_DELAY
        )
    return _anti_scraping_manager


def _get_browser_config() -> BrowserConfig:
    """è·å–æµè§ˆå™¨é…ç½®ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰
    
    ä¼˜åŒ–ï¼š
    - æ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
    - ä½¿ç”¨å®Œæ•´çš„ BrowserProfile å’Œ HTTP headers
    - æ•´åˆ AntiScrapingManager çš„é…ç½®
    """
    # åŸºç¡€æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼ˆé€‚ç”¨äºæ‰€æœ‰é…ç½®ï¼‰
    base_extra_args = [
        '--disable-blink-features=AutomationControlled',
        '--disable-infobars',
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-gpu',  # ç¦ç”¨ GPU åŠ é€Ÿï¼ˆheadless æ¨¡å¼ï¼‰
        '--disable-software-rasterizer',  # ç¦ç”¨è½¯ä»¶å…‰æ …åŒ–
        '--disable-extensions',  # ç¦ç”¨æ‰©å±•
        '--disable-plugins',  # ç¦ç”¨æ’ä»¶
        '--disable-images',  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼ˆæå‡é€Ÿåº¦ï¼‰
    ]
    
    if ANTI_SCRAPING_ENABLED:
        manager = _get_anti_scraping_manager()
        profile = manager.get_browser_profile()
        # è·å–å®Œæ•´çš„ HTTP è¯·æ±‚å¤´ï¼ˆåŒ…å« Accept-Languageã€Sec-Ch-Uaã€Referer ç­‰ï¼‰
        headers = manager.get_http_headers()
        
        # æ•´åˆ extra_argsï¼šåˆå¹¶åŸºç¡€å‚æ•°å’Œé˜²åçˆ¬å‚æ•°
        # æ³¨æ„ï¼šwindow-size å·²ç»åœ¨ viewport ä¸­è®¾ç½®ï¼Œä¸éœ€è¦é‡å¤
        extra_args = base_extra_args + [
            '--disable-setuid-sandbox',  # ä» get_browser_config() ä¸­æ·»åŠ 
        ]
        
        return BrowserConfig(
            headless=True,
            viewport_width=profile.viewport_width,
            viewport_height=profile.viewport_height,
            user_agent=profile.user_agent,
            headers=headers,  # æ·»åŠ å®Œæ•´çš„ HTTP è¯·æ±‚å¤´ï¼Œæå‡åæ£€æµ‹èƒ½åŠ›
            verbose=False,
            extra_args=extra_args
        )
    else:
        # é»˜è®¤é…ç½®ï¼ˆä¸å¯ç”¨é˜²åçˆ¬ï¼‰
        return BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=800,
            verbose=False,
            extra_args=base_extra_args
        )

def get_browser_config() -> BrowserConfig:
    """è·å–æµè§ˆå™¨é…ç½®ï¼ˆå…¬å¼€æ¥å£ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼‰"""
    return _get_browser_config()

def ensure_browser_config() -> BrowserConfig:
    """ç¡®ä¿æµè§ˆå™¨é…ç½®å·²åˆå§‹åŒ–ï¼ˆå…¬å¼€æ¥å£ï¼Œä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼‰"""
    return _ensure_browser_config()


# å…±äº«çš„æµè§ˆå™¨é…ç½®ï¼ˆå¤ç”¨ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å‡½æ•°åŠ¨æ€ç”Ÿæˆï¼Œæ”¯æŒé˜²åçˆ¬
# ä¼˜åŒ–ï¼šå¯¹äºé˜²åçˆ¬æ¨¡å¼ï¼Œæ¯æ¬¡è·å–æ–°é…ç½®ä»¥æ”¯æŒè½®æ¢ï¼›å¯¹äºéé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®
_SHARED_BROWSER_CONFIG = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆä»…ç”¨äºéé˜²åçˆ¬æ¨¡å¼ï¼‰


def _ensure_browser_config() -> BrowserConfig:
    """ç¡®ä¿æµè§ˆå™¨é…ç½®å·²åˆå§‹åŒ–
    
    ä¼˜åŒ–ï¼šå¯¹äºé˜²åçˆ¬æ¨¡å¼ï¼Œæ¯æ¬¡è·å–æ–°é…ç½®ä»¥æ”¯æŒè½®æ¢å’ŒæŒ‡çº¹ä¸€è‡´æ€§
    å¯¹äºéé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®ä»¥æå‡æ€§èƒ½
    """
    global _SHARED_BROWSER_CONFIG
    # å¦‚æœå¯ç”¨é˜²åçˆ¬ï¼Œæ¯æ¬¡éƒ½è·å–æ–°é…ç½®ï¼ˆæ”¯æŒè½®æ¢ï¼‰
    if ANTI_SCRAPING_ENABLED:
        return _get_browser_config()
    # éé˜²åçˆ¬æ¨¡å¼ï¼Œå¤ç”¨é…ç½®
    if _SHARED_BROWSER_CONFIG is None:
        _SHARED_BROWSER_CONFIG = _get_browser_config()
    return _SHARED_BROWSER_CONFIG


async def create_shared_crawler():
    """åˆ›å»ºå…±äº«çš„æµè§ˆå™¨å®ä¾‹ï¼ˆæ”¯æŒé˜²åçˆ¬ï¼‰
    
    ä¼˜åŒ–ï¼šä¼˜å…ˆä»æµè§ˆå™¨æ± è·å–ï¼Œå¦‚æœæ± å·²æ»¡åˆ™åˆ›å»ºç‹¬ç«‹å®ä¾‹
    """
    from .browser_pool import get_browser_pool
    browser_pool = get_browser_pool()
    
    # å°è¯•ä»æ± ä¸­è·å–
    crawler = await browser_pool.acquire()
    if crawler:
        return crawler
    
    # æ± å·²æ»¡ï¼Œåˆ›å»ºç‹¬ç«‹å®ä¾‹
    if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_ROTATE_UA:
        browser_config = _get_browser_config()
        logger.debug(f"ğŸ›¡ï¸ åˆ›å»ºé˜²åçˆ¬æµè§ˆå™¨å®ä¾‹ï¼ˆç‹¬ç«‹ï¼‰")
    else:
        browser_config = _ensure_browser_config()
    
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.__aenter__()
    return crawler

def parse_read_count(text: str) -> Optional[int]:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼Œå¤„ç† k/m/w åç¼€å’Œé€—å·åˆ†éš”ç¬¦
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - çº¯æ•°å­—: "1000" -> 1000
    - å¸¦é€—å·: "1,234" -> 1234
    - kåç¼€: "1k" -> 1000, "20k" -> 20000, "1.5k" -> 1500
    - måç¼€: "1m" -> 1000000, "2.5m" -> 2500000
    - wåç¼€: "1w" -> 10000, "10w" -> 100000
    - æ··åˆ: "1,234.5k" -> 1234500
    
    ç¤ºä¾‹ï¼š
        parse_read_count("1k") -> 1000
        parse_read_count("20k") -> 20000
        parse_read_count("1.5k") -> 1500
    """
    if not text:
        return None
    
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
    text = text.strip().replace(' ', '')
    
    # åŒ¹é…æ•°å­—ï¼ˆæ”¯æŒå°æ•°ç‚¹ã€é€—å·ï¼‰å’Œk/m/wåç¼€
    # æ¨¡å¼è¯´æ˜ï¼š
    #   [\d,]+         åŒ¹é…æ•°å­—å’Œé€—å·ï¼ˆæ•´æ•°éƒ¨åˆ†ï¼‰
    #   (?:\.[\d,]+)?  åŒ¹é…å¯é€‰çš„å°æ•°éƒ¨åˆ†ï¼ˆåŒ…å«å°æ•°ç‚¹ï¼‰
    #   ([kmwKMW]?)    åŒ¹é…å¯é€‰çš„åç¼€ï¼ˆk/m/wï¼Œå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    match = re.search(r'([\d,]+(?:\.[\d,]+)?)([kmwKMW]?)', text)
    if not match:
        return None
    
    number_str = match.group(1)
    suffix = match.group(2).lower()
    
    # ç§»é™¤æ‰€æœ‰é€—å·ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°
    number_str = number_str.replace(',', '')
    
    try:
        number = float(number_str)
    except ValueError:
        return None
    
    # åç¼€å€æ•°æ˜ å°„
    multipliers = {
        'k': 1000,      # åƒ: 1k = 1000, 20k = 20000
        'm': 1000000,   # ç™¾ä¸‡: 1m = 1000000
        'w': 10000      # ä¸‡ï¼ˆä¸­æ–‡ï¼‰: 1w = 10000
    }
    multiplier = multipliers.get(suffix, 1)
    
    # è®¡ç®—æœ€ç»ˆç»“æœå¹¶è½¬æ¢ä¸ºæ•´æ•°
    result = int(number * multiplier)
    return result

async def _crawl_with_shared(url: str, crawler: AsyncWebCrawler, crawler_config: CrawlerRunConfig):
    """ä½¿ç”¨å…±äº«æµè§ˆå™¨å®ä¾‹çˆ¬å–é¡µé¢ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    
    é›†æˆé˜²åçˆ¬åŠŸèƒ½ï¼šäººç±»åŒ–å»¶è¿Ÿ
    ä¼˜åŒ–ï¼šåŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„æ—¥å¿—
    """
    try:
        # æ‰§è¡Œäººç±»åŒ–å»¶è¿Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ANTI_SCRAPING_ENABLED and ANTI_SCRAPING_RANDOM_DELAY:
            manager = _get_anti_scraping_manager()
            await manager.human_delay()
        
        result = await crawler.arun(url, config=crawler_config)
        if not result.success:
            # è®°å½•å¤±è´¥åŸå› ï¼ˆå¦‚æœ result æœ‰é”™è¯¯ä¿¡æ¯ï¼‰
            error_msg = getattr(result, 'error', 'æœªçŸ¥é”™è¯¯')
            logger.debug(f"çˆ¬å–å¤±è´¥ {url}: {error_msg}")
            return None
        return result
    except asyncio.TimeoutError as e:
        logger.warning(f"â±ï¸ çˆ¬å–è¶…æ—¶ {url}: {e}")
        return None
    except ConnectionError as e:
        logger.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯ {url}: {e}")
        return None
    except Exception as e:
        # æ ¹æ®é”™è¯¯ç±»å‹åˆ†ç±»è®°å½•
        error_str = str(e).lower()
        if 'timeout' in error_str or 'timed out' in error_str:
            logger.warning(f"â±ï¸ è¶…æ—¶é”™è¯¯ {url}: {e}")
        elif 'connection' in error_str or 'network' in error_str:
            logger.warning(f"ğŸ”Œ ç½‘ç»œé”™è¯¯ {url}: {e}")
        elif 'ssl' in error_str or 'certificate' in error_str:
            logger.warning(f"ğŸ”’ SSLé”™è¯¯ {url}: {e}")
        else:
            logger.warning(f"âš ï¸ çˆ¬å–å¤±è´¥ {url}: {e}")
        return None

@lru_cache(maxsize=None)  # æ— ç•Œç¼“å­˜ï¼Œå› ä¸ºæ¨¡å¼æ•°é‡æœ‰é™ä¸”å›ºå®š
def _compile_pattern(pattern: str) -> re.Pattern:
    """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç¼“å­˜ç¼–è¯‘ç»“æœï¼Œæå‡æ€§èƒ½ï¼‰"""
    return re.compile(pattern, re.IGNORECASE | re.DOTALL)

def _parse_number(text: str, method: str = 'number') -> Optional[int]:
    """æ ¹æ®æŒ‡å®šæ–¹æ³•è§£ææ•°å­—
    
    Args:
        text: è¦è§£æçš„æ–‡æœ¬
        method: è§£ææ–¹æ³•
            - 'number': ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒk/m/wåç¼€ï¼‰
            - 'number_with_suffix': æ”¯æŒk/m/wåç¼€ï¼ˆå¦‚ 1k=1000, 20k=20000ï¼‰
    
    Returns:
        è§£æåçš„æ•´æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if not text:
        return None
    
    if method == 'number_with_suffix':
        # ä½¿ç”¨ parse_read_count å¤„ç†å¸¦åç¼€çš„æ•°å­—
        # æ³¨æ„ï¼šparse_read_count å†…éƒ¨ä¼šå¤„ç†ç©ºæ ¼å’Œé€—å·
        return parse_read_count(text)
    else:
        # ä»…æå–çº¯æ•°å­—ï¼ˆä¸æ”¯æŒåç¼€ï¼‰
        # ç§»é™¤ç©ºæ ¼å’Œé€—å·ï¼Œç„¶åæå–ç¬¬ä¸€ä¸ªæ•°å­—
        text = text.strip().replace(' ', '').replace(',', '')
        match = re.search(r'(\d+)', text)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                return None
        return None


# é¢„ç¼–è¯‘æ ‡é¢˜æå–çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
_TITLE_PATTERNS = {
    'title': re.compile(r'<title[^>]*>([^<]+)</title>', re.IGNORECASE),
    'h1': re.compile(r'<h1[^>]*>([^<]+)</h1>', re.IGNORECASE | re.DOTALL),
    'og_title1': re.compile(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']', re.IGNORECASE),
    'og_title2': re.compile(r'<meta[^>]*content=["\']([^"\']+)["\'][^>]*property=["\']og:title["\']', re.IGNORECASE),
}
_TITLE_SUFFIX_PATTERNS = [
    re.compile(r'\s*[-|_â€“â€”]\s*(æ˜é‡‘|CSDN|åšå®¢å›­|51CTO|SegmentFault|ç®€ä¹¦|ç”µå­å‘çƒ§å‹|ä¸éç½‘).*$', re.IGNORECASE),
    re.compile(r'\s*[-|_â€“â€”]\s*.*åšå®¢.*$', re.IGNORECASE),
    re.compile(r'\s*[-|_â€“â€”]\s*.*æŠ€æœ¯.*$', re.IGNORECASE),
]
_HTML_TAG_PATTERN = re.compile(r'<[^>]+>')

def _extract_title_from_html(html: str) -> Optional[str]:
    """ä» HTML ä¸­æå–æ–‡ç« æ ‡é¢˜ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. <title> æ ‡ç­¾
    2. <h1> æ ‡ç­¾
    3. og:title meta æ ‡ç­¾
    """
    if not html:
        return None
    
    # 1. å°è¯•ä» <title> æ ‡ç­¾æå–
    title_match = _TITLE_PATTERNS['title'].search(html)
    if title_match:
        title = title_match.group(1).strip()
        # æ¸…ç†å¸¸è§çš„ç½‘ç«™åç¼€ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
        for suffix_pattern in _TITLE_SUFFIX_PATTERNS:
            title = suffix_pattern.sub('', title)
        if title:
            return title.strip()
    
    # 2. å°è¯•ä» <h1> æ ‡ç­¾æå–
    h1_match = _TITLE_PATTERNS['h1'].search(html)
    if h1_match:
        title = h1_match.group(1).strip()
        # ç§»é™¤ HTML æ ‡ç­¾
        title = _HTML_TAG_PATTERN.sub('', title)
        if title:
            return title.strip()
    
    # 3. å°è¯•ä» og:title meta æ ‡ç­¾æå–
    og_match = _TITLE_PATTERNS['og_title1'].search(html)
    if og_match:
        return og_match.group(1).strip()
    
    # åå‘åŒ¹é… og:title
    og_match2 = _TITLE_PATTERNS['og_title2'].search(html)
    if og_match2:
        return og_match2.group(1).strip()
    
    return None


async def extract_with_config(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        é˜…è¯»æ•°ï¼Œå¤±è´¥è¿”å› None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return None
    
    # è°ƒç”¨å®Œæ•´ç‰ˆæœ¬ï¼Œåªè¿”å›é˜…è¯»æ•°
    read_count, _ = await extract_with_config_full(url, platform, crawler)
    return read_count


async def extract_with_config_full(url: str, platform: str, crawler: Optional[AsyncWebCrawler] = None) -> tuple:
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æå–é˜…è¯»æ•°å’Œæ ‡é¢˜
    
    Args:
        url: ç›®æ ‡URL
        platform: å¹³å°æ ‡è¯†ï¼ˆå¦‚ 'juejin', 'csdn'ï¼‰
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
    
    Returns:
        (é˜…è¯»æ•°, æ ‡é¢˜) å…ƒç»„ï¼Œå¤±è´¥æ—¶å¯¹åº”å€¼ä¸º None
    """
    if platform not in PLATFORM_EXTRACTORS:
        return (None, None)
    
    config = PLATFORM_EXTRACTORS[platform]
    patterns = config.get('patterns', [])
    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæå‡æ€§èƒ½ï¼‰
    # HTML ä½¿ç”¨ DOTALL æ¨¡å¼ï¼ˆæ”¯æŒè·¨è¡ŒåŒ¹é…ï¼‰ï¼Œmarkdown ä¸ä½¿ç”¨
    compiled_patterns_html = [_compile_pattern(p) for p in patterns]
    compiled_patterns_markdown = [re.compile(p, re.IGNORECASE) for p in patterns]
    wait_for = config.get('wait_for')
    timeout = config.get('timeout', 20000)
    parse_method = config.get('parse_method', 'number')
    delay_before_return = config.get('delay_before_return', 0)  # é¢å¤–å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    js_extract = config.get('js_extract', False)  # æ˜¯å¦ä½¿ç”¨ JavaScript æå–
    
    # è·å–é˜²åçˆ¬é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    base_crawler_config = {}
    js_parts = []
    
    if ANTI_SCRAPING_ENABLED:
        manager = _get_anti_scraping_manager()
        # è·å–åŸºç¡€é˜²åçˆ¬é…ç½®
        base_crawler_config = manager.get_crawler_config(
            timeout=timeout,
            wait_for=wait_for
        )
        # å¦‚æœé˜²åçˆ¬é…ç½®ä¸­æœ‰ js_codeï¼Œæ·»åŠ åˆ° js_parts
        if base_crawler_config.get('js_code'):
            js_parts.append(base_crawler_config['js_code'])
            # ç§»é™¤ js_codeï¼Œç¨ååˆå¹¶æ‰€æœ‰ JS ä»£ç 
            base_crawler_config.pop('js_code', None)
    
    # å¹³å°ç‰¹å®šçš„ JavaScript æå–é€»è¾‘
    if js_extract and platform == 'sohu':
        # æœç‹ï¼šwait_for å·²ç¡®ä¿æ•°å­—åŠ è½½å®Œæˆï¼Œè¿™é‡Œç›´æ¥æå–å¹¶æ³¨å…¥æ ‡è®°
        platform_js = """
        (() => {
            const pvEl = document.querySelector('em[data-role="pv"]');
            if (pvEl) {
                const text = pvEl.textContent.trim();
                if (/^\\d+$/.test(text)) {
                    // åœ¨ HTML ä¸­æ³¨å…¥æ˜ç¡®çš„æ ‡è®°ï¼Œç¡®ä¿èƒ½è¢«æ­£åˆ™æå–
                    const marker = document.createElement('script');
                    marker.type = 'text/plain';
                    marker.id = 'sohu-pv-marker';
                    marker.textContent = 'SOHU_PV_COUNT:' + text;
                    document.head.appendChild(marker);
                    console.log('Sohu PV injected:', text);
                    return text;
                }
            }
            return null;
        })();
        """
        js_parts.append(platform_js)
    
    # åˆå¹¶ JavaScript ä»£ç ï¼šå…ˆæ‰§è¡Œéšèº«è„šæœ¬ï¼Œå†æ‰§è¡Œå¹³å°è„šæœ¬
    combined_js = '\n'.join(js_parts) if js_parts else None
    
    # åˆ›å»ºçˆ¬å–é…ç½®ï¼ˆæ•´åˆé˜²åçˆ¬é…ç½®å’Œå¹³å°ç‰¹å®šé…ç½®ï¼‰
    crawler_config = CrawlerRunConfig(
        page_timeout=timeout,
        wait_for=wait_for,
        remove_overlay_elements=base_crawler_config.get('remove_overlay_elements', True),  # ç§»é™¤å¼¹çª—å’Œé®ç½©å±‚
        screenshot=base_crawler_config.get('screenshot', False),  # ç¦ç”¨æˆªå›¾ä»¥æå‡æ€§èƒ½
        js_code=combined_js if combined_js else None
    )
    
    # ä½¿ç”¨å…±äº«æµè§ˆå™¨æˆ–åˆ›å»ºæ–°å®ä¾‹
    if crawler:
        # ä½¿ç”¨ä¼ å…¥çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        result = await _crawl_with_shared(url, crawler, crawler_config)
        if result is None:
            return (None, None)
    else:
        # æ²¡æœ‰ä¼ å…¥ crawlerï¼Œå°è¯•ä»æµè§ˆå™¨æ± è·å–æˆ–åˆ›å»ºä¸´æ—¶å®ä¾‹
        from .browser_pool import get_browser_pool
        browser_pool = get_browser_pool()
        
        # å°è¯•ä»æ± ä¸­è·å–
        pool_crawler = await browser_pool.acquire()
        if pool_crawler:
            try:
                result = await _crawl_with_shared(url, pool_crawler, crawler_config)
                if result is None:
                    return (None, None)
            finally:
                # ç¡®ä¿é‡Šæ”¾å›æ± ä¸­
                await browser_pool.release(pool_crawler)
        else:
            # æ± å·²æ»¡ï¼Œåˆ›å»ºä¸´æ—¶å®ä¾‹ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿æ­£ç¡®æ¸…ç†ï¼‰
            browser_config = _ensure_browser_config()
            async with AsyncWebCrawler(config=browser_config) as temp_crawler:
                result = await temp_crawler.arun(url, config=crawler_config)
                if not result.success:
                    return (None, None)
    
    # å¦‚æœé…ç½®äº†é¢å¤–å»¶è¿Ÿï¼Œç­‰å¾… JavaScript æ¸²æŸ“
    if delay_before_return > 0:
        await asyncio.sleep(delay_before_return / 1000.0)  # è½¬æ¢ä¸ºç§’
    
    html = result.html
    markdown = result.markdown or ''
    
    # æ£€æµ‹éªŒè¯ç ï¼ˆéƒ¨åˆ†ç½‘ç«™çš„åçˆ¬æœºåˆ¶ï¼‰
    captcha_indicators = ['è®¿é—®éªŒè¯', 'è¯·æŒ‰ä½æ»‘å—', 'æ‹–åŠ¨åˆ°æœ€å³è¾¹', 'æ»‘å—éªŒè¯', 'CAPTCHA_DETECTED']
    for indicator in captcha_indicators:
        if indicator in html:
            logger.warning(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæ— æ³•æå–: {url}")
            return (None, None)
    
    # æå‰æå–æ–‡ç« æ ‡é¢˜
    article_title = _extract_title_from_html(html)
    
    # å¦‚æœé…ç½®äº† JavaScript æå–ï¼Œä¼˜å…ˆä»æ ‡è®°ä¸­æå–ï¼ˆæ”¯æŒ sohu ç­‰ï¼‰
    if js_extract:
        # æ–¹æ³•1: ä» READ_COUNT æ ‡è®°æå–
        title_match = re.search(r'READ_COUNT:([\d,]+)', html)
        if title_match:
            count = _parse_number(title_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        
        # æ–¹æ³•2: ä» SOHU_READ_COUNT æ ‡è®°æå–ï¼ˆæœç‹ä¸“ç”¨ï¼Œæ”¯æŒ HTML æ³¨é‡Šæ ¼å¼ï¼‰
        sohu_match = re.search(r'SOHU_READ_COUNT:([\d,]+)', html)
        if sohu_match:
            count = _parse_number(sohu_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
        
        # æ–¹æ³•3: ä» SOHU_PV_COUNT æ ‡è®°æå–ï¼ˆæœç‹ä¸“ç”¨ï¼‰
        sohu_pv_match = re.search(r'SOHU_PV_COUNT:(\d+)', html)
        if sohu_pv_match:
            count = _parse_number(sohu_pv_match.group(1), parse_method)
            if count is not None and count > 0:
                return (count, article_title)
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæ¨¡å¼ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    for i, compiled_pattern_html in enumerate(compiled_patterns_html):
        # å…ˆåœ¨ HTML ä¸­æŸ¥æ‰¾
        match = compiled_pattern_html.search(html)
        if match:
            text = match.group(1).strip()  # å»é™¤é¦–å°¾ç©ºç™½
            count = _parse_number(text, parse_method)
            if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                return (count, article_title)
        
        # å¦‚æœ HTML ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨ markdown ä¸­æŸ¥æ‰¾ï¼ˆä½¿ç”¨å¯¹åº”çš„é¢„ç¼–è¯‘æ¨¡å¼ï¼‰
        if markdown:
            compiled_pattern_md = compiled_patterns_markdown[i]
            match = compiled_pattern_md.search(markdown)
            if match:
                text = match.group(1)
                count = _parse_number(text, parse_method)
                if count is not None and count > 0:  # ç¡®ä¿ä¸æ˜¯ 0
                    return (count, article_title)
    
    # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥ï¼Œè¿”å› None
    return (None, article_title)


async def extract_article_info(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Dict[str, Any]:
    """æå–æ–‡ç« ä¿¡æ¯ï¼ˆé˜…è¯»æ•°å’Œæ ‡é¢˜ï¼‰
    
    Args:
        url: æ–‡ç«  URL
        crawler: å¯é€‰çš„å…±äº«æµè§ˆå™¨å®ä¾‹
        
    Returns:
        åŒ…å« 'read_count' å’Œ 'title' çš„å­—å…¸
    """
    from .config import SUPPORTED_SITES
    
    domain = urlparse(url).netloc.lower()
    
    # æ ¹æ®åŸŸååŒ¹é…å¹³å°ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ˜ å°„ï¼‰
    platform = None
    for site_domain, site_name in SUPPORTED_SITES.items():
        if site_domain in domain:
            platform = site_name
            break
    
    result = {'read_count': None, 'title': None}
    
    if platform and platform in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, platform, crawler)
    elif 'generic' in PLATFORM_EXTRACTORS:
        result['read_count'], result['title'] = await extract_with_config_full(url, 'generic', crawler)
    
    return result


async def extract_read_count(url: str, crawler: Optional[AsyncWebCrawler] = None) -> Optional[int]:
    """æ ¹æ®URLè‡ªåŠ¨é€‰æ‹©æå–å™¨ï¼ˆä»…è¿”å›é˜…è¯»æ•°ï¼‰"""
    info = await extract_article_info(url, crawler)
    return info.get('read_count')
